{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp nnpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNPU\n",
    "\n",
    "> Implementations of [Non-Negative Risk Estimator](https://arxiv.org/abs/1703.00593) and [AbsNNPU](https://papers.nips.cc/paper/2020/file/98b297950041a42470269d56260243a1-Paper.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import losses\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "from scipy.stats import bernoulli\n",
    "from easydict import EasyDict\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import scipy.stats as ss\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Basic(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, n_units, n_hidden, dropout_rate):\n",
    "        super(Basic, self).__init__()\n",
    "        self.Dens = list()\n",
    "        self.BN = list()\n",
    "        self.Drop = list()\n",
    "        for i in np.arange(n_hidden):\n",
    "            if i == 0:\n",
    "                self.Dens.append(layers.Dense(n_units, activation='relu'))\n",
    "            else:\n",
    "                self.Dens.append(layers.Dense(n_units, activation='relu'))\n",
    "            self.BN.append(layers.BatchNormalization())\n",
    "            self.Drop.append(layers.Dropout(dropout_rate))\n",
    "        self.dens_last = layers.Dense(1)\n",
    "        # self.BN_last = layers.BatchNormalization()\n",
    "        # self.sigmoid = activations.sigmoid()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        for i in np.arange(len(self.Dens)):\n",
    "            if i == 0:\n",
    "                x = self.Dens[i](inputs)\n",
    "            else:\n",
    "                x = self.Dens[i](x)\n",
    "            x = self.BN[i](x)\n",
    "            x = self.Drop[i](x)\n",
    "        x = self.dens_last(x)\n",
    "        # x = self.BN_last(x)\n",
    "        return activations.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def NNPULoss(alpha):\n",
    "    epsilon = 10 ** -10\n",
    "\n",
    "    def loss_function(y_true, pn_posterior):\n",
    "        i_zero = K.flatten(tf.equal(y_true, 0))\n",
    "        i_one = K.flatten(tf.equal(y_true, 1))\n",
    "        pn_posterior_0 = tf.boolean_mask(pn_posterior[:, 0], i_zero, axis=0)\n",
    "        pn_posterior_1 = tf.boolean_mask(pn_posterior[:, 0], i_one, axis=0)\n",
    "        loss_neg = -tf.reduce_mean(tf.math.log(1 - pn_posterior_0 + epsilon))\n",
    "        loss_neg = tf.maximum(0.0, loss_neg + alpha * tf.reduce_mean(tf.math.log(1 - pn_posterior_1 + epsilon)))\n",
    "        loss_pos = -alpha * tf.reduce_mean(tf.math.log(pn_posterior_1 + epsilon))\n",
    "        return loss_neg + loss_pos\n",
    "\n",
    "    return loss_function\n",
    "\n",
    "\n",
    "def NNPUAbsLoss(alpha):\n",
    "    epsilon = 10 ** -10\n",
    "\n",
    "    def loss_function(y_true, pn_posterior):\n",
    "        i_zero = K.flatten(tf.equal(y_true, 0))\n",
    "        i_one = K.flatten(tf.equal(y_true, 1))\n",
    "        pn_posterior_0 = tf.boolean_mask(pn_posterior[:, 0], i_zero, axis=0)\n",
    "        pn_posterior_1 = tf.boolean_mask(pn_posterior[:, 0], i_one, axis=0)\n",
    "        loss_neg = -tf.reduce_mean(tf.math.log(1 - pn_posterior_0 + epsilon))\n",
    "        loss_neg = tf.math.abs(loss_neg + alpha * tf.reduce_mean(tf.math.log(1 - pn_posterior_1 + epsilon)))\n",
    "        loss_pos = -alpha * tf.reduce_mean(tf.math.log(pn_posterior_1 + epsilon))\n",
    "        return loss_neg + loss_pos\n",
    "\n",
    "    return loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def gradients(net, x, y, LossFnc):\n",
    "    #YGen = np.cast['float32'](np.concatenate((y,pn_posterior_old, disc_posterior), axis=1))\n",
    "    with tf.GradientTape() as tape:\n",
    "        #pdb.set_trace()\n",
    "        loss = LossFnc(y, net(x))\n",
    "    return loss, tape.gradient(loss, net.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def batch(x, y, n_p, n_u):\n",
    "    x_p, ix_p = batchPos(x, y, n_p)\n",
    "    x_u, ix_u = batchUL(x, y, n_u)\n",
    "    xx = np.concatenate((x_p, x_u), axis=0)\n",
    "    ix = np.concatenate((ix_p, ix_u), axis=0)\n",
    "    return xx, y[ix, :], x_p, x_u, ix\n",
    "\n",
    "\n",
    "def batchPos(x, y, n_p):\n",
    "    return batchY(x, y, 1, n_p)\n",
    "\n",
    "\n",
    "def batchUL(x, y, n_u):\n",
    "    return batchY(x, y, 0, n_u)\n",
    "\n",
    "def batchY(x, y, value, n, *args):\n",
    "    ix = (y == value).flatten( )\n",
    "    ix_all = np.arange(np.size(y))\n",
    "    ix = ix_all[ix]\n",
    "    if args:\n",
    "        p = args[0].flatten()\n",
    "        p = p[ix]\n",
    "        ix_p = bernoulli.rvs(p)\n",
    "        ix_p = np.cast['bool'](ix_p)\n",
    "        ix = ix[ix_p]\n",
    "    ix = np.random.choice(ix, n, replace=True)\n",
    "    xx = x[ix, :]\n",
    "    return xx, ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def getPosterior(x,y,alpha,\n",
    "                 inputs=None,\n",
    "                 pupost=None,\n",
    "                 training_args=EasyDict({\"n_units\":1000,\n",
    "                                         \"n_hidden\":10,\n",
    "                                         \"dropout_rate\":0.5,\n",
    "                                         \"maxIter\":500,\n",
    "                                         \"batch_size\":128}),\n",
    "                 distributions=None,\n",
    "                 viz_freq=10,\n",
    "                 plotDistrs=False,\n",
    "                absLoss=True,\n",
    "                yPN=None):\n",
    "    \"\"\"\n",
    "    x : (n x d) array\n",
    "    y : (n x 1) array\n",
    "    alpha : float\n",
    "    training_args: EasyDict\n",
    "        n_units : default 20 : size of hiddden layers\n",
    "        n_hidden : default 10 : number of hidden layers\n",
    "        dropout_rate : default 0.1 : drop percentage\n",
    "        maxIter : default 100 : number of epochs\n",
    "        batch_size : default 500 : batch size\n",
    "    distributions : EasyDict :\n",
    "        true_posterior(x) : callable\n",
    "        f1(x) : callable\n",
    "        f0(x) : callable\n",
    "    viz_freq : default 10 : if distributions is specified, plot the 1D distributions at this period\n",
    "    \"\"\"\n",
    "    # model\n",
    "    net = Basic(training_args.n_units,\n",
    "                training_args.n_hidden,\n",
    "                training_args.dropout_rate)\n",
    "    # loss\n",
    "    if absLoss:\n",
    "        LossFnc = NNPUAbsLoss(alpha)\n",
    "    else:\n",
    "        LossFnc = NNPULoss(alpha)\n",
    "    # optimizer\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    if pupost is not None:\n",
    "        inputs = pupost(x)\n",
    "    elif inputs is not None:\n",
    "        inputs = inputs\n",
    "    else:\n",
    "        inputs = x\n",
    "    inputsTrain,inputsVal,labelsTrain,labelsVal = train_test_split(inputs,y)\n",
    "    \n",
    "    def plot():\n",
    "        estimatedPosterior = net.predict(inputs)[:,0].ravel()\n",
    "        truePosterior = distributions.true_posterior(x).ravel()\n",
    "        plt.scatter(estimatedPosterior, truePosterior,alpha=.1,label=\"mae: {:.3f}\".format(np.mean(np.abs(estimatedPosterior - truePosterior))))\n",
    "        plt.plot([0,1],[0,1],color=\"black\")\n",
    "        plt.xlabel(\"estimated posterior\")\n",
    "        plt.ylabel(\"true posterior\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    minLoss,patience = np.inf,0\n",
    "    for i in tqdm(range(training_args.maxIter),total=training_args.maxIter, leave=False):\n",
    "        xx,yy,_,_,ix = batch(inputsTrain,labelsTrain,training_args.batch_size,training_args.batch_size)\n",
    "        loss, grads = gradients(net,xx,yy,LossFnc)\n",
    "        opt.apply_gradients(zip(grads, net.trainable_variables))\n",
    "        valloss,_ = gradients(net,inputsVal, labelsVal,LossFnc)\n",
    "        if valloss < minLoss:\n",
    "            minLoss = valloss\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "        if distributions is not None and not i % viz_freq:\n",
    "            plot()\n",
    "        if patience == 50:\n",
    "            break\n",
    "    if distributions is not None:\n",
    "        plot()\n",
    "    return net.predict(inputs),net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiInstanceLearning.data.gaussian_dg import GaussianMixtureDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = GaussianMixtureDataGenerator(64,1,[0.99,.999],1000,10000)\n",
    "\n",
    "[xPU,yPU,yPN] = dg.pu_data()\n",
    "\n",
    "xPUTrain,xPUVal,yPUTrain,yPUVal = train_test_split(xPU,yPU,test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getPosterior(xPU,yPU,dg.alpha,\n",
    "             distributions=EasyDict({\"true_posterior\" : dg.pn_posterior_cc,\n",
    "                                     \"f1\" : dg.dens_pos,\n",
    "                                     \"f0\" : dg.dens_neg}),\n",
    "             viz_freq=50,\n",
    "             absLoss=False,\n",
    "             pupost=None,\n",
    "             yPN=yPN,\n",
    "             training_args=EasyDict({\"n_units\": 1000,\n",
    "                                     \"n_hidden\":3,\n",
    "                                     \"dropout_rate\":0.5,\n",
    "                                     \"maxIter\":10000,\n",
    "                                     \"batch_size\":128}),)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
