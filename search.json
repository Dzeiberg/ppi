[
  {
    "objectID": "13_FoldX_Analysis.html",
    "href": "13_FoldX_Analysis.html",
    "title": "ppi",
    "section": "",
    "text": "import networkx as nx\nedgotype = nx.read_gexf(\"data/y2hEdgotyping/edgotype.gefx\")\n# edgotype_train = nx.read_gpickle(\"data/y2hEdgotyping/edgotype_train.gpickle\")\n# edgotype_val = nx.read_gpickle(\"data/y2hEdgotyping/edgotype_val.gpickle\")\n# edgotype_test = nx.read_gpickle(\"data/y2hEdgotyping/edgotype_test.gpickle\")\n\n\n\nimport os\nimport pandas as pd\nseqFiles = [pd.read_csv(f\"data/y2hEdgotyping/uniprotScan/sequence_{i}.tsv\",delimiter=\"\\t\") for i in range(6)]\n\nuniprotMatches = pd.concat(seqFiles)\ndef mergeWithUniprot(graph):\n    for node in graph.nodes(data=True):\n        seq = node[1][\"seq\"]\n        up = uniprotMatches[(uniprotMatches.Sequence == seq) & \\\n                            (uniprotMatches.Reviewed == \"reviewed\") & \\\n                           (uniprotMatches.Organism == \"Homo sapiens (Human)\")]\n        graph.nodes[node[0]][\"uniprotMatches\"] = up\n        alphafoldStructures = []\n        for uniprot_id in graph.nodes[node[0]][\"uniprotMatches\"][\"Entry\"]:\n            fp = f\"/data/dzeiberg/alphafold/predictions/AF-{uniprot_id}-F1-model_v4.pdb.gz\"\n            if os.path.isfile(fp):\n                alphafoldStructures.append(fp)\n        graph.nodes[node[0]][\"alphafoldStructures\"] = alphafoldStructures\n    return graph\n\n\nedgotype_x = mergeWithUniprot(edgotype)\n\n\npaths = []\nfor ensg,n in edgotype_x.nodes(data=True):\n    seq = n[\"seq\"]\n    p = f\"/data/dzeiberg/ppi/alphafold/{ensg}.faa\"\n    paths.append(p)\n    with open(p,\"w\") as faa:\n        faa.write(f\">{ensg}\\n{seq}\\n\")\n\n\n\",\".join(paths)\n\n\nnode2Vars = {}\nfor i,j,e in edgotype_x.edges(data=True):\n    db = e[\"db_ensembl_gene_id_mt\"]\n    if db not in node2Vars:\n        node2Vars[db] = set()\n    node2Vars[db].add(e[\"Substitution\"])\n\n\npdbs = set()\nfor _,e in edgotype_x.nodes(data=True):\n    pdb = e[\"uniprotMatches\"].PDB.values\n    if len(pdb) and type(pdb[0]) is str:\n        pdb = pdb[0].split(\";\")\n        pdbs.update(pdb[:-1])\n\n\nedgotype_x.nodes[\"ENSG00000223609\"]\n\n\nmuts = list(node2Vars[\"ENSG00000223609\"])\nwith open(\"/home/dzeiberg/test_rosetta/1si4.mutfile\",\"w\") as f:\n    f.write(str(len(muts)))\n    f.write(\"\\n\")\n    for v in muts:\n        f.write(\"1\\n\")\n        loc = v[1:-1]\n        mut = v[-1]\n        f.write(f\"{loc} A PIKAA {mut}\\n\")\n\n\nwith open(\"/data/dzeiberg/ppi/y2hEdgotyping/foldX/pdb_id_list.txt\",\"w\") as f:\n    f.write(\",\".join(pdbs))\n\n\nimport os\nimport shutil\n\n\nimport subprocess\n\n\nfor db,varstrs in node2Vars.items():\n    pdb = edgotype_x.nodes[db][\"uniprotMatches\"].PDB\n    if len(pdb.values) and type(pdb.values[0]) is str:\n        pdb_ids = pdb.values[0].strip(\";\").split(\";\")\n        for i,p_id in enumerate(pdb_ids):\n            if os.path.isfile(os.path.join(\"/data/dzeiberg/ppi/y2hEdgotyping/foldX/pdb_files/\",p_id+\".pdb\")):\n                p_dir = os.path.join(\"/data/dzeiberg/ppi/y2hEdgotyping/foldX/data\",db+f\"_struct_{i}\")\n                if not os.path.isdir(p_dir):\n                    os.mkdir(p_dir)\n                shutil.copy(os.path.join(\"/data/dzeiberg/ppi/y2hEdgotyping/foldX/pdb_files/\",p_id+\".pdb\"), p_dir)\n                subprocess.run([\"/data/utilities/bio/foldX/foldx_20231231\", \"--command=SequenceOnly\", f\"--pdb={p_id}.pdb\"],\n                                cwd=p_dir,check=True)\n                with open(os.path.join(p_dir,f\"SO_{p_id}.fxout\")) as so:\n                    pdb_seq = so.readlines()[1]\n                with open(os.path.join(p_dir,\"config_1.cfg\"),\"w\") as f:\n                    f.write(f\"\"\"command=RepairPDB\n    pdb={p_id}.pdb\"\"\")\n                validvars = [v for v in varstrs if pdb_seq[int(v[1])] == v[0]]\n                if len(validvars):\n                    positions = \",\".join([v[0]+\"A\"+v[1:] for v in validvars])\n                    with open(os.path.join(p_dir,\"config_2.cfg\"),\"w\") as f:\n                        f.write(f\"\"\"command=PositionScan\n        pdb={p_id}.pdb\n        positions={positions}\"\"\")\n\n\nos.path.isfile(os.path.join(p_dir,f\"SO_{p_id}.fxout\"))\n\n\nimport pandas as pd\n\n\nli = []\nfor n in edgotype_train.nodes(data=True):\n    m = n[1][\"uniprotMatches\"]\n#     print(m.PDB.values)\n    if len(m.PDB.values) and type(m.PDB.values[0]) is str:\n        li.append(m.PDB.values[0].strip(\";\").split(\";\"))\n\n\npdbids = sum(li,[])\n\n\nwith open(\"/data/dzeiberg/ppi/y2hEdgotyping/foldX/pdb_ids.txt\",\"w\") as f:\n    f.write(\",\".join(pdbids))"
  },
  {
    "objectID": "13_Language_Model.html",
    "href": "13_Language_Model.html",
    "title": "ppi",
    "section": "",
    "text": "import torch\nfrom transformers import AlbertModel, AlbertTokenizer\nimport re\nimport os\nimport requests\nfrom tqdm.auto import tqdm\n\n\n\ntokenizer = AlbertTokenizer.from_pretrained(\"Rostlab/prot_albert\", do_lower_case=False)\n\n\nmodel = AlbertModel.from_pretrained(\"Rostlab/prot_albert\")\n\n\ndevice = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\nprint(device)\n\n\nmodel = model.to(device)\nmodel = model.eval()\n\n\nimport networkx as nx\nedgotype = nx.read_gexf(\"data/y2hEdgotyping/edgotype.gefx\")\n\n\nimport os\nimport pandas as pd\nseqFiles = [pd.read_csv(f\"data/y2hEdgotyping/uniprotScan/sequence_{i}.tsv\",delimiter=\"\\t\") for i in range(6)]\n\nuniprotMatches = pd.concat(seqFiles)\ndef mergeWithUniprot(graph):\n    for node in graph.nodes(data=True):\n        seq = node[1][\"seq\"]\n        up = uniprotMatches[(uniprotMatches.Sequence == seq) & \\\n                            (uniprotMatches.Reviewed == \"reviewed\") & \\\n                           (uniprotMatches.Organism == \"Homo sapiens (Human)\")]\n        graph.nodes[node[0]][\"uniprotMatches\"] = up\n        alphafoldStructures = []\n        for uniprot_id in graph.nodes[node[0]][\"uniprotMatches\"][\"Entry\"]:\n            fp = f\"/data/dzeiberg/alphafold/predictions/AF-{uniprot_id}-F1-model_v4.pdb.gz\"\n            if os.path.isfile(fp):\n                alphafoldStructures.append(fp)\n        graph.nodes[node[0]][\"alphafoldStructures\"] = alphafoldStructures\n    return graph\nedgotype_x = mergeWithUniprot(edgotype)\n\n\ndef makeMut(seq,mut):\n    og,loc,var = mut[0],int(mut[1:-1]) - 1,mut[-1]\n    return seq[:loc] + var + seq[loc+1:]\n\n\nsequences,ensg_ids,substitutions = list(zip(*[(\" \".join(list(makeMut(edgotype_x.nodes(data=True)[e[\"db_ensembl_gene_id_mt\"]][\"seq\"],\n                                                        e[\"Substitution\"]))),\n                                   e[\"db_ensembl_gene_id_mt\"],\n                                   e[\"Substitution\"]) for _,_,e in edgotype_x.edges(data=True)]))\n\n\n# sequences,ensg_ids = list(zip(*[(\" \".join(list(n[\"seq\"])),ensg) for ensg,n in edgotype_x.nodes(data=True)]))\n\n\nsequences = [re.sub(r\"[UZOB]\", \"X\", sequence) for sequence in sequences]\n\n\nlen(sequences)\n\n\nlen(sub)\n\n\nids = tokenizer.batch_encode_plus(sequences, add_special_tokens=True, padding='longest')\n\n\ninput_ids = torch.tensor(ids['input_ids']).to(device)\nattention_mask = torch.tensor(ids['attention_mask']).to(device)\n\n\nimport torch.utils.data as data_utils\n\nds = data_utils.TensorDataset(input_ids,attention_mask)\nloader = data_utils.DataLoader(ds, batch_size=8, shuffle=False)\n\n\nlen(ds)\n\n\nlen(loader)\n\n\nembeddings = []\nfor (inp_id, inp_att_mask) in tqdm(loader,total=len(loader)):\n    with torch.no_grad():    \n        embeddings.append(model(input_ids=inp_id,attention_mask=inp_att_mask)[0].cpu().numpy())\n\n\nembeddingsMat = np.concatenate(embeddings)\n\n\nfeatures = [] \nfor seq_num in range(len(embeddingsMat)):\n    seq_len = (attention_mask[seq_num] == 1).sum()\n    seq_emd = embeddingsMat[seq_num][1:seq_len-1]\n    features.append(seq_emd)\n\n\nfor ensg,sub_i,f in zip(ensg_ids,substitutions,features):\n    fp = f\"/data/dzeiberg/ppi/y2hEdgotyping/protAlbertEmbeddings/{ensg}_{sub_i}.pt\"\n    print(fp)\n    torch.save(f,fp)\n\n\ndef calc_score(e):\n    score = 0\n    for med in [\"LWH1_f_\",\"LWH10_f_\", \"LWH25_f_\",\n                \"LWA_f_\",\"LWAH1_f_\"]:\n        score +=  e[med+\"wt\"] - e[med+\"mt\"]\n    return score\n\ndef calc_label(e):\n    label = False\n    for med in [\"LWH1_f_\",\"LWH10_f_\", \"LWH25_f_\",\n                \"LWA_f_\",\"LWAH1_f_\"]:\n        label = label or (e[med+\"wt\"] - 2 >= e[med+\"mt\"])\n    return label\n\n\nimport os\n\n\nfiles = []\nscores = []\nlabels = []\nfor i,j,edge in edgotype_x.edges(data=True):\n    db = edge[\"db_ensembl_gene_id_mt\"]\n    mut = edge[\"Substitution\"]\n    ad = edge[\"ad_ensembl_gene_id_mt\"]\n    fi = f\"/data/dzeiberg/ppi/y2hEdgotyping/protAlbertEmbeddings/{db}_{mut}.pt\"\n    fj = f\"/data/dzeiberg/ppi/y2hEdgotyping/protAlbertEmbeddings/{ad}.pt\"\n    if os.path.isfile(fi) and os.path.isfile(fj):\n        files.append((fi,fj))\n        scores.append(calc_score(edge))\n        labels.append(calc_label(edge))\n    else:\n        print(fi,os.path.isfile(fi),fj,os.path.isfile(fj))\n\n\ndef loadInst(fi,fj):\n    Xi = torch.load(fi).mean(0)\n    Xj = torch.load(fj).mean(0)\n    return np.multiply(Xi,Xj)\n\n\nX = []\nfor (fi,fj) in tqdm(files):\n    X.append(loadInst(fi,fj))\n\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import Ridge, LinearRegression,LogisticRegression\nfrom sklearn.svm import SVC\n\n\nXMat = np.stack(X)\n\n\nXMat.shape\n\n\nlabels = np.array(labels)\n\n\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn.metrics import roc_auc_score\n\n\nfig,ax = plt.subplots(4,5,figsize=(12,8),sharex=True)\nvalIndices = []\nvalPreds = []\naucs = []\nfor i,(trainInds, valInds) in enumerate(KFold().split(XMat,labels)):\n    XTrain = XMat[trainInds]\n    yTrain = labels[trainInds]\n    tmask = ~np.isnan(yTrain)\n    XTrain = XTrain[tmask]\n    yTrain = yTrain[tmask]\n    XVal = XMat[valInds]\n    yVal = labels[valInds]\n    vmask = ~np.isnan(yVal)\n    XVal,yVal = XVal[vmask],yVal[vmask]\n#     model = Ridge()\n    model = LogisticRegression(C=.1,max_iter=1000)\n#     model = SVC(probability=True)\n    model.fit(XTrain,yTrain)\n#     yHat = model.predict(XVal)\n    yHat = model.predict_proba(XVal)[:,1]\n#     print(np.mean(np.abs(yVal.ravel() - yHat.ravel())))\n    aucs.append(roc_auc_score(yVal.ravel(),yHat.ravel()))\n    print(aucs[-1])\n    valIndices.append(valInds)\n    valPreds.append(yHat)\n    yTHat = model.predict_proba(XTrain)[:,1]\n    ax[0,i].hist(yTHat[yTrain])\n    ax[1,i].hist(yTHat[~yTrain])\n    ax[2,i].hist(yHat[yVal])\n    ax[3,i].hist(yHat[~yVal])\nprint(np.mean(aucs))"
  },
  {
    "objectID": "10_MutPred2_Stability.html",
    "href": "10_MutPred2_Stability.html",
    "title": "ppi",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport glob\nimport os\n\n\n\nmp2 = pd.read_csv(\"/data/dzeiberg/ppi/y2hEdgotyping/mutpred2Results_v2/variants.faa.out\")\n\n\nmp2 = mp2.assign(mm=mp2[\"Molecular mechanisms with Pr >= 0.01 and P < 1.00\"])\n\n\nimport re\ndef getFloats(s):\n    numeric_const_pattern = '[-+]? (?: (?: \\d* \\. \\d+ ) | (?: \\d+ \\.? ) )(?: [Ee] [+-]? \\d+ ) ?'\n    rx = re.compile(numeric_const_pattern,re.VERBOSE)\n    return [float(f) for f in rx.findall(s)]\n\n\ndef getStability(res,prob=True):\n    property_pr_p = [(si[:si.find(\"(\")],getFloats(si))for si in sorted([s.strip() for s in res.split(\";\")])]\n    for (prop,vals) in property_pr_p:\n        pr,p = vals[-2:]\n        if prop == \"Altered Stability \":\n            if prob:\n                return pr\n            return p\n\n\nimport networkx as nx\nedgotype_train = nx.read_gpickle(\"data/y2hEdgotyping/edgotype_train.gpickle\")\n\n\ne = next(iter(edgotype_train.edges(data=True)))\n\n\ndef make_y2h_target(d):\n    names = [\"LWH1_f_\",\"LWH10_f_\",\"LWH25_f_\",\n             \"LWA_f_\",\"LWAH1_f_\"]\n    deltas = np.zeros(len(names))\n    for i,name in enumerate(names):\n        deltas[i] = d[name+\"wt\"] - d[name+\"mt\"]\n    return np.any(deltas >= 2)\n\n\nprobs,labels = list(zip(*[(getStability(e[2][\"Molecular mechanisms with Pr >= 0.01 and P < 0.99\"]),\n                                    make_y2h_target(e[2])) for e in edgotype_train.edges(data=True)]))\n\n\nsum([p is None for p in probs])\n\n\nlen(probs)\n\n\nlabels = np.array([labels[i] for i in range(len(labels)) if probs[i] is not None])\nprobs = np.array([p for p in probs if p is not None])\n\n\nimport matplotlib.pyplot as plt\n\n\nplt.hist(probs[labels],density=True)\nplt.hist(probs[~labels],alpha=.5,density=True)\n\n\nfrom scipy.io import loadmat\n\n\nm = loadmat(\"/data/utilities/bio/mutpred2_source/all_models/model_Stability_081416.mat\")\n\n\nm.keys()\n\n\nm[\"nn_model\"].shape\n\n\nm[\"features\"].shape"
  },
  {
    "objectID": "variant_affects_any_edge.html",
    "href": "variant_affects_any_edge.html",
    "title": "Load Data",
    "section": "",
    "text": "import networkx as nx\nimport pandas as pd"
  },
  {
    "objectID": "variant_affects_any_edge.html#variants-on-wt-proteins-with-multiple-partners",
    "href": "variant_affects_any_edge.html#variants-on-wt-proteins-with-multiple-partners",
    "title": "Load Data",
    "section": "Variants on WT proteins with multiple partners",
    "text": "Variants on WT proteins with multiple partners\n\nroc_auc_score(table[table.n_edges > 1].y, \n              table[table.n_edges > 1][\"MutPred2 score\"])\n\n\nplt.hist(table[(table.y) & \\\n               (table.n_edges > 1)][\"MutPred2 score\"].values,\n         label=\"Edgetic/Quasi-null\",density=True)\nplt.hist(table[(~table.y) & \\\n               (table.n_edges > 1)][\"MutPred2 score\"].values,\n         color=\"red\",alpha=.5,label=\"Quasi-WT\",density=True)\nplt.legend()\nplt.xlabel(\"MutPred2 Score\")\nplt.ylabel(\"Density\")"
  },
  {
    "objectID": "variant_affects_any_edge.html#all-variants-1",
    "href": "variant_affects_any_edge.html#all-variants-1",
    "title": "Load Data",
    "section": "All variants",
    "text": "All variants\n\nTrain\n\nroc_auc_score(variantTableTrain.y, \n              variantTableTrain[\"MutPred2 score\"])\n\n\n\nVal\n\nroc_auc_score(variantTableVal.y, \n              variantTableVal[\"MutPred2 score\"])\n\n\n\nTest\n\nroc_auc_score(variantTableTest.y, \n              variantTableTest[\"MutPred2 score\"])"
  },
  {
    "objectID": "variant_affects_any_edge.html#only-variants-on-wt-nodes-with-single-partner",
    "href": "variant_affects_any_edge.html#only-variants-on-wt-nodes-with-single-partner",
    "title": "Load Data",
    "section": "Only variants on WT nodes with single partner",
    "text": "Only variants on WT nodes with single partner\n\nTrain\n\nroc_auc_score(variantTableTrain[variantTableTrain.n_edges == 1].y, \n              variantTableTrain[variantTableTrain.n_edges == 1][\"MutPred2 score\"])\n\n\n\nVal\n\nroc_auc_score(variantTableVal[variantTableVal.n_edges == 1].y, \n              variantTableVal[variantTableVal.n_edges == 1][\"MutPred2 score\"])\n\n\n\nTest\n\nroc_auc_score(variantTableTest[variantTableTest.n_edges == 1].y, \n              variantTableTest[variantTableTest.n_edges == 1][\"MutPred2 score\"])"
  },
  {
    "objectID": "variant_affects_any_edge.html#only-variants-on-wt-nodes-with-multiple-partners",
    "href": "variant_affects_any_edge.html#only-variants-on-wt-nodes-with-multiple-partners",
    "title": "Load Data",
    "section": "Only variants on WT nodes with multiple partners",
    "text": "Only variants on WT nodes with multiple partners\n\nTrain\n\nroc_auc_score(variantTableTrain[variantTableTrain.n_edges > 1].y, \n              variantTableTrain[variantTableTrain.n_edges > 1][\"MutPred2 score\"])\n\n\n\nVal\n\nroc_auc_score(variantTableVal[variantTableVal.n_edges > 1].y, \n              variantTableVal[variantTableVal.n_edges > 1][\"MutPred2 score\"])\n\n\n\nTest\n\nroc_auc_score(variantTableTest[variantTableTest.n_edges > 1].y, \n              variantTableTest[variantTableTest.n_edges > 1][\"MutPred2 score\"])\n\n\nnext(iter(edgotype_train.edges(data=True)))"
  },
  {
    "objectID": "12_Write_Fasta_for_Alphafold.html",
    "href": "12_Write_Fasta_for_Alphafold.html",
    "title": "ppi",
    "section": "",
    "text": "import networkx as nx\n\n\n\nedgotype_train = nx.read_gpickle(\"data/y2hEdgotyping/edgotype_train.gpickle\")\nedgotype_val = nx.read_gpickle(\"data/y2hEdgotyping/edgotype_val.gpickle\")\nedgotype_test = nx.read_gpickle(\"data/y2hEdgotyping/edgotype_test.gpickle\")\n\n\nlen(edgotype_train.edges()) + len(edgotype_val.edges())# + len(edgotype_test.edges())\n\n\n2784 * 3.5 / 24\n\n\nsyms = [\"PTK2\",\"SRC\",\n        \"LMNB1\",\"LMNA\",\n       'NFE2L1',\"MAFG\",\n       \"CDK2\",\"CKS1B\",\n       \"JUNB\",\"BATF\",\n       \"CASP9\", \"XIAP\",\n       \"MAD2L1\",\"MAD1L1\",\n       \"CRADD\",\"CASP2\",\n       \"LSM3\",\"LSM2\"]\n\n\n[n for n in edgotype_train.nodes(data=True) \\\n if n[1][\"symbol\"] in syms]\n\n\n[n[1][\"symbol\"] for n in edgotype_val.nodes(data=True) \\\n if n[1][\"symbol\"] in syms]\n\n\n[(n[1][\"symbol\"],n[0]) for n in edgotype_test.nodes(data=True) \\\n if n[1][\"symbol\"] in syms]\n\n\nmuts = list(edgotype_test.edges([\"ENSG00000169372\", \"ENSG00000106144\"],\n                    data=True))\n\n\ne = muts[0][2]\n\n\nd = edgotype_test.nodes[e[\"db_ensembl_gene_id_mt\"]]\ndb = d[\"seq\"]\n\n\nd[\"symbol\"]\n\n\na = edgotype_test.nodes[e[\"ad_ensembl_gene_id_mt\"]]\nad = a[\"seq\"]\n\n\na[\"symbol\"]\n\n\nvarstr = e[\"aa_change_mt\"]\n\n\nog,loc,var = varstr[:3],int(varstr[3:-3]) - 1, varstr[-3:]\n\n\naaPairs = {\"A\":\"Ala\", \"R\":\"Arg\", \"N\":\"Asn\", \"D\":\"Asp\",\n           \"C\":\"Cys\", \"E\":\"Glu\", \"Q\":\"Gln\", \"G\":\"Gly\",\n           \"H\":\"His\", \"I\":\"Ile\", \"L\":\"Leu\", \"K\":\"Lys\",\n           \"M\":\"Met\", \"F\":\"Phe\", \"P\":\"Pro\", \"S\":\"Ser\",\n           \"T\":\"Thr\", \"W\":\"Trp\", \"Y\":\"Tyr\", \"V\":\"Val\"}\n\naaTable = dict(list(zip(*list(zip(*aaPairs.items()))[::-1])))\n\n\nwith open(\"/data/dzeiberg/tmp/testAlphafold/wt/input.fasta\",\"w\") as f:\n    f.write(f\">db\\n{db}\\n>ad\\n{ad}\")\n\n\nvar\n\n\ndb_mut = db[:loc] + aaTable[var] + db[loc + 1:]\n\n\ndb_mut\n\n\nwith open(\"/data/dzeiberg/tmp/testAlphafold/mt/input.fasta\",\"w\") as f:\n    f.write(f\">db\\n{db_mut}\\n>ad\\n{ad}\")\n\n\ncat /data/dzeiberg/tmp/testAlphafold/wt/input.fasta"
  },
  {
    "objectID": "20_Calibrate.html",
    "href": "20_Calibrate.html",
    "title": "ppi",
    "section": "",
    "text": "Notes:\n\nz-score normalize WT interactions, then also apply to MT\ngraph partitioned cross validation\nbootstrap MT for PPI pairs\n\nestimate distributions in projected space (PCA,Autoencoder,etc.)\nplot projected space\n\nEvaluation: - Does normalization affect prior? - Is there consistency across feature representations?\n\n\nfrom itertools import chain\n\n\nimport pandas as pd\nedgotype_df = pd.read_csv(\"data/y2hEdgotyping/qY2H_edgotyping_data.csv\",index_col=0)\n\n\nedgotype_df.clinical_significance.value_counts()\n\n\nwt_df = edgotype_df[edgotype_df.aa_change == \"WT\"]\nmt_df = edgotype_df[edgotype_df.aa_change != \"WT\"]\n\n\nwt_df[[\"db_ensembl_gene_id\",\"db_symbol\",\"ad_ensembl_gene_id\",\"ad_symbol\",\"aa_change\",\"clinical_significance\"]]\n\n\ndef getScores(wt):\n    scoreColumns = ['LWH1_f', 'LWH10_f', 'LWH25_f', 'LWA_f', 'LWAH1_f']\n    nameColumns = [\"db_symbol\",\"ad_symbol\",\"aa_change\",\"clinical_significance\"]\n    mts = mt_df[(mt_df.db_orf_id == wt[\"db_orf_id\"]) & \\\n                (mt_df.ad_orf_id == wt[\"ad_orf_id\"])]\n    score_wt = wt[scoreColumns].values.astype(float).reshape((1,-1))\n    name_wt = wt[nameColumns]\n    if np.isnan(score_wt).any():\n        return np.zeros((0,5)),np.zeros((0,5)),[],[]\n    s_mts = mts[scoreColumns].dropna(axis=0)\n    score_mts = s_mts.values.astype(float)\n    if len(s_mts.index):\n        _,name_mts = zip(*mts.loc[s_mts.index, nameColumns].iterrows())\n    else:\n        name_mts = []\n    return score_wt, score_mts, [name_wt], name_mts\n\n\nimport numpy as np\n\n\ndef encode_scores(scores):\n    encoded = np.zeros((scores.shape[0],25))\n    for i,s in enumerate(scores):\n        for j,sj in enumerate(s):\n            encoded[i,5 * j + int(sj)] = 1\n    return encoded\n\n\nENCODE = False\nif ENCODE:\n    dim = 25\n    penalty='l2'\nelse:\n    dim = 5\n    penalty=None\nscore_wt = np.zeros((0,dim))\nscore_mt = np.zeros((0,dim))\nnames_wt = []\nnames_mt = []\nfor wt_id, wt in wt_df.iterrows():\n    score_wt_i, score_mt_i,name_wt,name_mts = getScores(wt)\n    assert len(score_mt_i) == len(name_mts)\n    if ENCODE:\n        score_wt_i = encode_scores(score_wt_i)\n        score_mt_i = encode_scores(score_mt_i)\n    score_wt = np.concatenate((score_wt, score_wt_i))\n    score_mt = np.concatenate((score_mt,score_mt_i))\n    names_wt.append(name_wt)\n    names_mt.append(name_mts)\n\n\nscore_wt.shape\n\n\nscore_wt\n\n\nwt_means = score_wt.mean(1)\n\n\nplt.hist(wt_means,bins=25)\n\n\nscore_wt\n\n\nscore_mt.shape\n\n\nX = np.concatenate((score_wt, score_mt))\ny = np.concatenate((np.ones(score_wt.shape[0]),\n                    np.zeros(score_mt.shape[0])))\n\n\nnamerecords = list(chain.from_iterable(names_wt + names_mt))\nnames = pd.DataFrame(namerecords,index=range(len(namerecords)))\n\n\nnames.loc[[4484,4485]]\n\n\nnames\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold\n\n\nimport matplotlib.pyplot as plt\n\n\nimport dist_curve\nfrom dist_curve.curve_constructor import makeCurve, plotCurve\nfrom dist_curve.model import getTrainedEstimator\n\n\nmodel = getTrainedEstimator(\"/data/dzeiberg/ClassPriorEstimation/model.hdf5\")\n\n\nfrom sklearn.metrics import roc_auc_score\n\n\nfrom ppi.nnpu import getPosterior\n\n\nclfs = []\naucs = []\npriors = []\nfig,ax = plt.subplots(2,5,figsize=(12,4),sharex=True,sharey=True)\nfig2,ax2 = plt.subplots(2,5,figsize=(16,3))\nfor i,(trainInd,testInd) in enumerate(KFold(shuffle=True).split(X,y)):\n    print(f\"~~~~~~~~~~ Fold {i} ~~~~~~~~~~\")\n    XTr,yTr = X[trainInd],y[trainInd]\n    XTe, yTe = X[testInd],y[testInd]\n    names_Te = names.iloc[testInd]\n    clf_i = LogisticRegression(penalty=\"l2\")\n    clf_i.fit(XTr,yTr)\n#     posScores = clf_i.predict_proba(XTe[yTe == 0])[:,1].reshape((-1,1))\n#     mixScores = clf_i.predict_proba(XTe[yTe == 1])[:,1].reshape((-1,1))\n    scores = clf_i.predict_proba(XTe)[:,1]\n    posScores = scores[yTe == 1].reshape((-1,1))\n    mixScores = scores[yTe == 0].reshape((-1,1))\n    auc = roc_auc_score(yTe,scores)\n    aucs.append(auc)\n    print(\"AUC: {:.3f}\".format(auc))\n    n,bins,patches = ax[0,i].hist(posScores,\n               bins=10,density=True)\n    _,_,_ = ax[1,i].hist(mixScores,\n                      bins=bins,density=True,alpha=.5)\n    ax[0,i].set_title(f\"Fold-{i+1} Positive\")\n    ax[1,i].set_title(f\"Fold-{i+1} Unlabeled\")\n    clfs.append(clf_i)\n    curve_i = makeCurve(posScores,mixScores)\n    ax2[0,i].plot(np.arange(0,1,.01),\n                (curve_i - curve_i.min()) / (curve_i.max() - curve_i.min()))\n    alpha_i = model.predict(curve_i.reshape((1,-1)) / curve_i.sum(),\n                           verbose=0)[0,0]\n    print(f\"prior est: {alpha_i:.3f}\")\n    ax2[0,i].axvline(alpha_i,0,1)\n    ax2[0,i].set_title(f\"Fold {i+1}\")\n    priors.append(alpha_i)\n    train_preds,net_i = getPosterior(XTr,yTr.reshape((-1,1)),alpha_i)\n    test_preds = net_i.predict(XTe)\n    \n    print(names_Te.iloc[np.argsort(test_preds.ravel())][:5])\n    ax2[1,i].hist(test_preds)\nprint(f\"Average AUC_PU: {np.mean(aucs):.2f}\")\nprint(f\"Average Prior: {np.mean(priors):.2f}\")"
  },
  {
    "objectID": "alignments.html",
    "href": "alignments.html",
    "title": "many nodes didn’t have an AlphafoldKB match, but only 1 had a misalligned sequence",
    "section": "",
    "text": "import gzip\n\nfrom Bio.PDB.PDBParser import PDBParser\nfrom Bio.PDB.Polypeptide import PPBuilder\nppb=PPBuilder()\ndef getSeq(pdb):\n    for pp in ppb.build_peptides(pdb):\n        yield str(pp.get_sequence())\n\nimport networkx as nx\n\nedgotype = nx.read_gexf(\"data/y2hEdgotyping/edgotype.gefx\")\n\nimport os\nimport pandas as pd\nseqFiles = [pd.read_csv(f\"data/y2hEdgotyping/uniprotScan/sequence_{i}.tsv\",delimiter=\"\\t\") for i in range(6)]\n\nuniprotMatches = pd.concat(seqFiles)\n\nimport Bio\nimport Bio.PDB\nimport Bio.SeqRecord\n\npdbparser = Bio.PDB.PDBParser(QUIET=False,)   # suppress PDBConstructionWarning\nfrom tqdm import tqdm\ndef mergeWithUniprot(graph):\n    for node in tqdm(graph.nodes(data=True),total=len(graph.nodes())):\n        seq = node[1][\"seq\"]\n        up = uniprotMatches[(uniprotMatches.Sequence == seq) & \\\n                            (uniprotMatches.Reviewed == \"reviewed\") & \\\n                           (uniprotMatches.Organism == \"Homo sapiens (Human)\")]\n        graph.nodes[node[0]][\"uniprotMatches\"] = up\n        alphafoldStructures = []\n        for uniprot_id in graph.nodes[node[0]][\"uniprotMatches\"][\"Entry\"]:\n            fp = f\"/data/dzeiberg/alphafold/predictions/AF-{uniprot_id}-F1-model_v4.pdb.gz\"\n            if os.path.isfile(fp):\n                with gzip.open(fp,\"rt\",encoding='utf-8') as gz:\n                    struc = pdbparser.get_structure(fp,gz)\n                alphafoldStructures.append(struc)\n        graph.nodes[node[0]][\"alphafoldStructures\"] = alphafoldStructures\n        \n    return graph\n\nedgotype = mergeWithUniprot(edgotype)\n\n\nnodes = list(edgotype.nodes(data=True))\n\n\nfrom itertools import chain\n\n\nbadNodes = []\ngoodNodes = []\nfor ensg,node in nodes:\n    if not len(node[\"alphafoldStructures\"]):\n        badNodes.append(ensg)\n        print(f\"no prediction for {ensg}\")\n        continue\n    if node[\"seq\"] not in list(chain(*[list(getSeq(s)) for s in node[\"alphafoldStructures\"]])):\n        badNodes.append(ensg)\n        print(ensg)\n    else:\n        goodNodes.append(ensg)\n\n\nlen(goodNodes)\n\n\nlen(nodes)\n\n\nlen(badNodes)\n\n\nlen(badNodes)\n\n\nnomatchseqs = []\nmatchnodes = []\nfor ensg in badNodes:\n    n = edgotype.nodes(data=True)[ensg]\n    if not len(n[\"uniprotMatches\"]):\n        nomatchseqs.append((ensg,n[\"seq\"]))\n    else:\n        matchnodes.append(ensg)\n\n\nlen(nomatchseqs)\n\n\nnomatchseqs[0]\n\n\n# for tup in nomatchseqs[:100]:\n#     print(tup[1])\n\n\n# for tup in nomatchseqs[100:]:\n#     print(tup[1])\n\n\nlen(badNodes),len(nomatchseqs),len(matchnodes)\n\n\nfor m in matchnodes:\n    node = edgotype.nodes(data=True)[m]\n    print(m, node[\"uniprotMatches\"].Entry.values[0],node[\"uniprotMatches\"].AlphaFoldDB.values)\n\n\nedgotype.nodes(data=True)[\"ENSG00000185303\"][\"alphafoldStructures\"]\n\n\nlist(getSeq(edgotype.nodes(data=True)[\"ENSG00000185303\"][\"alphafoldStructures\"][0]))\n\n\nedgotype.nodes(data=True)[\"ENSG00000185303\"][\"seq\"]\n\n\ndef addSubs(graph):\n    for n in graph.nodes():\n        edges = graph.edges(n,data=True)\n        db_edges = [e for e in edges if e[2][\"db_ensembl_gene_id_mt\"] == n]\n        subs = list(set([e[2][\"Substitution\"] for e in db_edges]))\n        nx.set_node_attributes(graph,{n: {\"subs\":subs}})\n    return graph\n\n\nedgotype = addSubs(edgotype)\n\n\nnodes = list(edgotype.nodes(data=True))\n\n\nlen(nodes[0][1][\"subs\"])\n\n\nfor n in nodes:\n    match = np.ones(len(n[1][\"subs\"])).astype(bool)\n    for i,s in enumerate(n[1][\"subs\"]):\n        og,loc,var = s[0],int(s[1:-1])-1, s[-1]\n        if n[1][\"seq\"][loc] != og:\n            match[i] = False\n    nx.set_node_attributes(edgotype,{n[0]: {\"match\":match}})\n\n\nfor ensg,n in edgotype.nodes(data=True):\n    if len(n[\"match\"]):\n        print(n[\"match\"].sum(), len(n[\"match\"]))\n\n\nseq = next(iter(edgotype.nodes(data=True)))[1][\"seq\"]\n\n\nseq"
  },
  {
    "objectID": "14_SDM2_Stability_Prediction_Correlation.html",
    "href": "14_SDM2_Stability_Prediction_Correlation.html",
    "title": "ppi",
    "section": "",
    "text": "import networkx as nx\n\nedgotype_train = nx.read_gpickle(\"data/y2hEdgotyping/edgotype_train.gpickle\")\nedgotype_val = nx.read_gpickle(\"data/y2hEdgotyping/edgotype_val.gpickle\")\nedgotype_test = nx.read_gpickle(\"data/y2hEdgotyping/edgotype_test.gpickle\")\n\n\n\nensg_id = \"ENSG00000123700\"\n\n\nGet edges corresponding to mutations to this gene\n\nedgeList = [e for e in edgotype_train.edges(\"ENSG00000123700\",data=True) if e[2][\"db_ensembl_gene_id_mt\"] == ensg_id]\n\n\n\nList of Partners\n\nset([e[2][\"ad_ensembl_gene_id_mt\"] for e in edgeList])\n\n\ndef getPPILabel(e):\n    y = np.zeros(5)\n    for i,med in enumerate([\"LWH1_f_\", \"LWH10_f_\",\n               \"LWH25_f_\", \"LWA_f_\",\"LWAH1_f_\"]):\n        s_wt = e[med+\"wt\"]\n        s_mt = e[med+\"mt\"]\n        if not (np.isnan(s_wt) or np.isnan(s_mt)):\n            y[i] = s_wt - s_mt\n    print(y)\n    return np.all(y[2:] > 1)\n\n\nmut2Edge = {}\nfor e in edgeList:\n    s = e[2][\"Substitution\"]\n    if s not in mut2Edge:\n        mut2Edge[s] = []\n    mut2Edge[s].append((*e,getPPILabel(e[2])))\n\n\nfor m,edges in mut2Edge.items():\n    print(m,[e[-1] for e in edges])"
  },
  {
    "objectID": "05_DeepFRI_Features.html",
    "href": "05_DeepFRI_Features.html",
    "title": "ppi",
    "section": "",
    "text": "import os\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3\"\n\n\n\nimport sys\n\nimport tensorflow as tf\n\n\ntf.config.list_physical_devices('GPU')\n\n\ntf.__version__\n\n\nsys.path.append(\"/home/dzeiberg/DeepFRI/\")\n\n\nfrom deepfrier.Predictor import Predictor\n\n\nDATA_DIR = \"/data/dzeiberg/DeepFRI/data/\"\n\n\nimport os\n\nimport json\n\n\nwith open(os.path.join(DATA_DIR,\"trained_models/model_config.json\")) as json_file:\n    params = json.load(json_file)\n\n\npredictor = Predictor(os.path.join(DATA_DIR,params[\"gcn\"][\"models\"][\"bp\"]),gcn=True)\n\n\nfeatModel = tf.keras.Model(inputs=predictor.model.input,\n                           outputs=predictor.model.layers[-4].output)\n\n\nimport networkx as nx\n\n\nedgotype_train = nx.read_gpickle(\"data/y2hEdgotyping/edgotype_train.gpickle\")\nedgotype_val = nx.read_gpickle(\"data/y2hEdgotyping/edgotype_val.gpickle\")\nedgotype_test = nx.read_gpickle(\"data/y2hEdgotyping/edgotype_test.gpickle\")\n\n\nnodeLabels = {n:[] for n in edgotype_train.nodes()}\nfor u,v,edge in edgotype_train.edges(data=True):\n    id_ = sorted([u,v])[0]\n    y = (edge[\"LWAH1_f_wt\"] - edge[\"LWAH1_f_mt\"]) >= 2\n    nodeLabels[id_].append(y)\n    \nnodeLabelsVal = {n:[] for n in edgotype_val.nodes()}\nfor u,v,edge in edgotype_val.edges(data=True):\n    id_ = sorted([u,v])[0]\n    y = (edge[\"LWAH1_f_wt\"] - edge[\"LWAH1_f_mt\"]) >= 2\n    nodeLabelsVal[id_].append(y)\n\nI should make this node pair specific instead of grouped across all partners and variants\n\nplt.scatter(*zip(*[(np.log10(len(adjEdges)),np.mean(adjEdges)) for adjEdges in nodeLabels.values() if len(adjEdges)]))\nplt.scatter(*zip(*[(np.log10(len(adjEdges)),np.mean(adjEdges)) for adjEdges in nodeLabelsVal.values() if len(adjEdges)]),alpha=.5)\nplt.xlabel(r\"$log_{10}(deg(n))$\")\nplt.ylabel(\"Edge Prior\")\n\n\nimport scipy\nfrom tqdm.notebook import tqdm\ndef makeFeats(G,FEATURE_SET=\"mutpredFeatures\"):\n    CMT = 10.0\n    X = []\n    y = []\n    for edgeNum,(ensg_i,ensg_j,edge) in tqdm(enumerate(G.edges(data=True)),\n                                   total=G.number_of_edges()):\n        node_i = G.nodes[ensg_i]\n        node_j = G.nodes[ensg_j]\n        yij = np.any([(edge[f\"{lvl}_wt\"] - edge[f\"{lvl}_mt\"]) >= 2 for lvl in [\"LWH1_f\",\n                                                                              \"LWH10_f\",\n                                                                              \"LWH25_f\",\n                                                                              \"LWA_f\",\n                                                                              \"LWAH1_f\"]])\n        y.append(yij)\n        if FEATURE_SET == \"deepFRI\" and len(node_i[\"alphafoldStructures\"]) and len(node_j[\"alphafoldStructures\"]):\n            Ai,Si,seq_i = predictor._load_cmap(node_i[\"alphafoldStructures\"][0],\n                                               cmap_thresh=CMT)\n            Aj,Sj,seq_j = predictor._load_cmap(node_j[\"alphafoldStructures\"][0],\n                                               cmap_thresh=CMT)\n            Xi = featModel([Ai,Si],training=False)\n            Xj = featModel([Aj,Sj],training=False)\n            X.append(np.concatenate((Xi,Xj),axis=-1))\n        elif FEATURE_SET == \"mutpredFeatures\":\n            fnum = str(int(edge[\"featFileNum\"]))\n            pth = f\"/data/dzeiberg/ppi/y2hEdgotyping/mutpred2Results/variants.faa.out.feats_{fnum}\"\n            MPFeats = scipy.io.loadmat(pth)[\"feats\"]\n            mutationFeat = np.array(MPFeats[int(edge[\"fileRowNum\"])]).reshape((1,-1))\n            X.append(mutationFeat)\n        elif FEATURE_SET == \"mutpredScore\":\n            X.append(np.array([edge[\"MutPred2 score\"]]).reshape((1,-1)))\n    X = np.concatenate(X)\n    return X,np.array(y).astype(float)\n\n\nXTrain,yTrain = makeFeats(edgotype_train,FEATURE_SET=\"mutpredScore\")\n\n\nXTrain.shape,yTrain.shape\n\n\nXVal,yVal = makeFeats(edgotype_val,FEATURE_SET=\"mutpredScore\")\n\n\nXVal.shape,yVal.shape\n\n\nfrom sklearn.preprocessing import StandardScaler\n\n\nscaler = StandardScaler()\nXTr = scaler.fit_transform(XTrain)\nXV = scaler.transform(XVal)\n\n\nimport tensorflow as tf\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n\nimport itertools\n\n\nif XTr.shape[1] == 1:\n    print(roc_auc_score(yVal,XV),roc_auc_score(yTrain,XTr))\n\n\n# MODEL = \"nn\"\n# MODEL = \"rf\"\nMODEL = \"lr\"\n\n\nif MODEL == \"rf\":\n    clf = RandomForestClassifier(n_jobs=16)\n    clf.fit(XTr,yTrain)\nelif MODEL == \"lr\":\n    clf = LogisticRegression(max_iter=1000)\n    clf.fit(XTr,yTrain)\nelif MODEL == \"nn\":\n    clf = tf.keras.Sequential(sum(itertools.repeat((tf.keras.layers.Dense(64),\n                                 tf.keras.layers.BatchNormalization(),\n                                 tf.keras.layers.ReLU()),3),())+ (tf.keras.layers.Dense(1,activation=None),))\n\n    clf.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n               metrics=[tf.keras.metrics.AUC(from_logits=True)])\n\n    clf.fit(XTr,yTrain,\n            validation_data=(XVal,yVal),\n            batch_size=32,epochs=100)\nelse:\n    raise ValueError(\"Invalid Model\")\n\n\nfrom sklearn.metrics import roc_auc_score\n\n\nvalPreds = clf.predict_proba(XV)[:,1]\n\n\nroc_auc_score(yVal, valPreds)\n\n\nimport matplotlib.pyplot as plt\n\n\nplt.hist(valPreds[yVal.astype(bool)])\nplt.hist(valPreds[~yVal.astype(bool)],color=\"red\",alpha=.5)"
  },
  {
    "objectID": "18_VESPA_data.html",
    "href": "18_VESPA_data.html",
    "title": "ppi",
    "section": "",
    "text": "import networkx as nx\n\nedgotype = nx.read_gexf(\"data/y2hEdgotyping/edgotype.gefx\")\n\n\n\nimport pandas as pd\nimport numpy as np\nimport itertools\n\n\nnext(iter(edgotype.nodes(data=True)))\n\n\nwith open(\"/data/dzeiberg/ppi/y2hEdgotyping/vespa/edgotype_seqs.fasta\",\"w\") as f:\n    for ensg_id,n in edgotype.nodes(data=True):\n        seq = n[\"seq\"]\n        f.write(f\">{ensg_id}\\n{seq}\\n\")\n\n\nmuts = set()\nfor _,_,e in edgotype.edges(\"ENSG00000002822\",data=True):\n    if e[\"db_ensembl_gene_id_mt\"] == \"ENSG00000002822\":\n        muts.add(e[\"Substitution\"])\n    else:\n        print(e[\"db_ensembl_gene_id_mt\"])\n\n\nlen(edgotype.nodes())\n\n\nmuts"
  },
  {
    "objectID": "mutpred_correlation.html",
    "href": "mutpred_correlation.html",
    "title": "Measuring the Correlation on MutPred2 Pathogenicity Score and Y2H Score Change",
    "section": "",
    "text": "import pandas as pd\n\n\nimport seaborn as sns\n\n\ndef validateSeqs(r):\n    ref,loc,alt = aaTable[r.aa_change_mt[:3]],int(r.aa_change_mt[3:-3]),aaTable[r.aa_change_mt[-3:]]\n    return (r.p_seq_db[loc-1] == ref) and (ref != alt)\n\n\naaPairs = {\"A\":\"Ala\", \"R\":\"Arg\", \"N\":\"Asn\", \"D\":\"Asp\",\n           \"C\":\"Cys\", \"E\":\"Glu\", \"Q\":\"Gln\", \"G\":\"Gly\",\n           \"H\":\"His\", \"I\":\"Ile\", \"L\":\"Leu\", \"K\":\"Lys\",\n           \"M\":\"Met\", \"F\":\"Phe\", \"P\":\"Pro\", \"S\":\"Ser\",\n           \"T\":\"Thr\", \"W\":\"Trp\", \"Y\":\"Tyr\", \"V\":\"Val\"}\n\naaTable = dict(list(zip(*list(zip(*aaPairs.items()))[::-1])))\n\n\nmp = pd.read_csv(\"data/y2hEdgotyping/mutpred2Results/variants.faa.out\")\n\n\nmp"
  },
  {
    "objectID": "mutpred_correlation.html#read-in-y2h-pre-processed-version-from-nb_01",
    "href": "mutpred_correlation.html#read-in-y2h-pre-processed-version-from-nb_01",
    "title": "Measuring the Correlation on MutPred2 Pathogenicity Score and Y2H Score Change",
    "section": "Read in Y2H pre-processed version from nb_01",
    "text": "Read in Y2H pre-processed version from nb_01\n\ny2h = pd.read_csv(\"data/y2hEdgotyping/y2HMerged.csv\",index_col=0)\n\n\nGet the degree counts of each “central node”\n\ny2h.groupby(by=[\"db_orf_id\",\"aa_change_mt\"]).ad_orf_id.count().sort_values()#.hist()"
  },
  {
    "objectID": "mutpred_correlation.html#merge-the-mutpred2-and-y2h-dataframes",
    "href": "mutpred_correlation.html#merge-the-mutpred2-and-y2h-dataframes",
    "title": "Measuring the Correlation on MutPred2 Pathogenicity Score and Y2H Score Change",
    "section": "Merge the MutPred2 and Y2H dataframes",
    "text": "Merge the MutPred2 and Y2H dataframes\n\nmp = mp.assign(db_orf_id=mp.ID.apply(lambda s: s.replace(\"db_orf_\",\"\")),\n               aa_change_mt=mp.Substitution.apply(lambda s: aaPairs[s[0]]+s[1:-1]+aaPairs[s[-1]]))\n\n\nmp\n\n\ny2h = y2h.assign(db_orf_id=y2h.db_orf_id.astype(str))\n\n\ny2h\n\n\ndf = pd.merge(y2h[y2h.apply(validateSeqs,axis=1)],mp,\n              left_on=[\"db_orf_id\",\"aa_change_mt\"],right_on=[\"db_orf_id\",\"aa_change_mt\"],\n              how=\"inner\")\n\n\ndf"
  },
  {
    "objectID": "mutpred_correlation.html#parse-probabilities-and-p-values-of-the-functional-effects-of-each-variant-estimated-by-mutpred2",
    "href": "mutpred_correlation.html#parse-probabilities-and-p-values-of-the-functional-effects-of-each-variant-estimated-by-mutpred2",
    "title": "Measuring the Correlation on MutPred2 Pathogenicity Score and Y2H Score Change",
    "section": "Parse probabilities and p-values of the functional effects of each variant estimated by MutPred2",
    "text": "Parse probabilities and p-values of the functional effects of each variant estimated by MutPred2\n\nimport re\n\n\ndef getFloats(s):\n    numeric_const_pattern = '[-+]? (?: (?: \\d* \\. \\d+ ) | (?: \\d+ \\.? ) )(?: [Ee] [+-]? \\d+ ) ?'\n    rx = re.compile(numeric_const_pattern,re.VERBOSE)\n    return [float(f) for f in rx.findall(s)]\n\n\nppiVals = []\nfor s in df[\"Molecular mechanisms with Pr >= 0.01 and P < 0.99\"]:\n    ppiVals.append(dict([(si[:si.find(\"(\")].strip(),getFloats(si)) for si in s.split(\";\") if \"Altered PPI_residue\" in si or \\\n                        \"Altered PPI_hotspot\" in si or \"Altered MoRF\" in si]))\n\n\ndf = df.assign(ppiVals=ppiVals)\n\n\ndf\n\n\ndf.groupby([\"db_orf_id\",\"aa_change_mt\"]).count()\n\n\ndf.ad_ensembl_gene_id_mt"
  },
  {
    "objectID": "mutpred_correlation.html#prepare-for-plotting",
    "href": "mutpred_correlation.html#prepare-for-plotting",
    "title": "Measuring the Correlation on MutPred2 Pathogenicity Score and Y2H Score Change",
    "section": "Prepare for Plotting",
    "text": "Prepare for Plotting\n\ndef getVal(d,key):\n    if key in d:\n        return d[key][0]\n    return np.nan\n\n\ngrp = df.groupby([\"db_orf_id\",\"aa_change_mt\"])\nGAIN_OR_LOSS = False\nvals = np.zeros((len(grp),15))\nkeys = []\nfor i,(idx,g) in enumerate(grp):\n    keys.append(idx)\n    for j in range(1,6):\n        if GAIN_OR_LOSS:\n            vals[i,j-1] = (np.abs(g[f\"delta{j}\"]) >= 2).mean()\n            vals[i,5+j-1] = (np.abs(g[f\"delta{j}\"]) >= 2).sum()\n        else:\n            vals[i,j-1] = (g[f\"delta{j}\"] >= 2).mean()\n            vals[i,5+j-1] = (g[f\"delta{j}\"] >= 2).sum()\n    vals[i,-5] = np.nanmean(g[\"ppiVals\"].apply(lambda d: getVal(d,\"Altered PPI_residue\")))\n    vals[i,-4] = np.nanmean(g[\"ppiVals\"].apply(lambda d: getVal(d,\"Altered PPI_hotspot\")))\n    vals[i,-3] = np.nanmean(g[\"ppiVals\"].apply(lambda d: getVal(d,\"Altered MoRF\")))\n    vals[i,-2] = g[\"MutPred2 score\"].mean()\n    vals[i,-1] = len(g)\n\n\nstatDF = pd.DataFrame(vals,columns=[f\"fD{i}\" for i in range(5)]+[f\"numD{i}\" for i in range(5)] + [\"AlteredPPIResiduePr\",\n                                                                                                  \"AlteredPPIHotspotPr\",\n                                                                                                  \"AlteredMoRFPr\",\n                                                                                                  \"MutPred2 score\",\n                                                                                                  \"db_degree\"],\n                      index=pd.MultiIndex.from_tuples(keys))\n\n\nstatDF.to_csv(\"data/y2hEdgotyping/variantStatistics.csv\")\n\n\nstatDF\n\n\nstatDF.numD2.value_counts().sort_index()\n\n\ndef mpDigitize(mpScore):\n    bins = np.ones_like(mpScore).astype(float) * np.nan\n    for i,s in enumerate(mpScore):\n        if s <= .01:\n            bins[i] = 0\n        elif s <= .197:\n            bins[i] = 1\n        elif s <= .391:\n            bins[i] = 2\n        elif s < .737:\n            bins[i] = 3\n        elif s < .829:\n            bins[i] = 4\n        elif s < .932:\n            bins[i] = 5\n        elif s >= .932:\n            bins[i] = 6\n        else:\n            raise ValueError(f\"Couldn't bin value {s}\")\n    return bins.astype(int)\n\n\nNBINS=5\nMINDEGREE = 1\nMAXDEGREE = np.inf\nsuffDf = statDF[(statDF.db_degree >= MINDEGREE) & (statDF.db_degree < MAXDEGREE)].dropna()\nmpPercentiles = np.percentile(suffDf[\"MutPred2 score\"],\n                        np.arange(100/NBINS,100,100/NBINS))\naltResPercentiles = np.nanpercentile(suffDf[\"AlteredPPIResiduePr\"],\n                        np.arange(100/NBINS,100,100/NBINS))\naltHotPercentiles = np.nanpercentile(suffDf[\"AlteredPPIHotspotPr\"],\n                        np.arange(100/NBINS,100,100/NBINS))\naltMoRFPercentiles = np.nanpercentile(suffDf[\"AlteredMoRFPr\"],\n                        np.arange(100/NBINS,100,100/NBINS))\n\nsuffDf = suffDf.assign(MutPred2ScoreBin=np.digitize(suffDf[\"MutPred2 score\"],\n                                                   mpPercentiles,right=True),\n                      MutPred2PaperBin=mpDigitize(suffDf[\"MutPred2 score\"]),\n                      alteredResidueBin=np.digitize(suffDf[\"AlteredPPIResiduePr\"],\n                                                   altResPercentiles, right=True),\n                      alteredHotspotBin=np.digitize(suffDf[\"AlteredPPIHotspotPr\"],\n                                                   altHotPercentiles, right=True),\n                      alteredMoRFBin=np.digitize(suffDf[\"AlteredMoRFPr\"],\n                                                   altMoRFPercentiles, right=True))\n\n\nfrom matplotlib.colors import ListedColormap\n\n\ncm = ListedColormap(sns.color_palette(\"GnBu\", 4))"
  },
  {
    "objectID": "mutpred_correlation.html#plot",
    "href": "mutpred_correlation.html#plot",
    "title": "Measuring the Correlation on MutPred2 Pathogenicity Score and Y2H Score Change",
    "section": "Plot",
    "text": "Plot\n\ndef makePlot(gb,ax,xlab,ticks,lgndLoc=\"upper left\",width=.75,byCount=False,lgnd=False,yLab=False):\n    if byCount:\n        counts = np.zeros((len(gb),4))\n        numDCutoffs = [1,5,10]\n        indices = []\n        for i,(idx,g) in enumerate(gb):\n            indices.append(idx)\n            groupBins = np.digitize(g,numDCutoffs,right=True)\n            binNum,binCount = np.unique(groupBins,return_counts=True)\n            counts[i,binNum] += binCount\n        for binNum,binHeights in enumerate(counts.T):\n            if binNum == 0:\n                lbl = \"0\"\n            elif binNum == len(numDCutoffs):\n                lbl = \"10+\"\n            else:\n                lbl = f\"({numDCutoffs[binNum-1]},{numDCutoffs[binNum]}]\"\n            ax.bar(np.array(indices) + ((binNum/4.0) - .5) * width,\n                  height=binHeights,width=width/4,\n                  color=cm(binNum),label=lbl)\n    else:\n        lowCut,highCut = np.arange(0,1,.25),np.arange(.25,1.25,.25)\n        highCut[-1] = 1.01\n        counts = [gb.aggregate(lambda fracs: ((fracs >= c[0]) & (fracs < c[1])).sum()) for c in zip(lowCut,highCut)]\n        for i,height in enumerate(counts,start=0):\n            if i == len(counts)-1:\n                lbl = \"[0.75,1.0]\"\n            else:\n                lbl = f\"[{lowCut[i]},{highCut[i]})\"\n            ax.bar(height.index +((i/4.0) - .5) * width,\n                      height=height.values,width=width/4,\n                     color=cm(i),label=lbl)\n    ax.set_xlabel(xlab)\n    if yLab:\n        ax.set_ylabel(\"Number Mutations\")\n    \n    ax.set_xticks(np.arange(len(ticks)).astype(int))\n    ax.set_xticklabels(ticks)\n    ax.set_xlim(-1,len(ticks))\n    handles, labels = ax.get_legend_handles_labels()\n    if lgnd:\n        box = ax.get_position()\n        ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n        ax.legend(handles[::-1], labels[::-1],loc='center left', bbox_to_anchor=(1, 0.5))\n\n    #     ax.legend(, loc=lgndLoc)\n\n\nfor d in range(5):\n    fig,ax = plt.subplots(1,5,figsize=(24,6),sharey=True)\n    \n#     makePlot(suffDf.groupby(\"MutPred2ScoreBin\")[f\"fD{d}\"],ax[0],\"MutPred2 Score Quantile\",\n#             np.arange(NBINS),\"upper right\")\n    \n#     makePlot(suffDf.groupby(\"MutPred2PaperBin\")[f\"fD{d}\"],ax[1],\"ClinGen SVI WG Recommendation\",\n#             [\"B3\",\"B2\",\"B1\",\"-\",\"P1\",\"P2\",\"P3\"])\n    \n#     makePlot(suffDf.groupby(\"alteredResidueBin\")[f\"fD{d}\"],ax[2],\"Altered PPI Residue Quantile\",\n#             np.arange(NBINS))\n    \n#     makePlot(suffDf.groupby(\"alteredHotspotBin\")[f\"fD{d}\"],ax[3],\"Altered PPI Hotspot Quantile\",\n#             np.arange(NBINS))\n    \n#     makePlot(suffDf.groupby(\"alteredMoRFBin\")[f\"fD{d}\"],ax[4],\"Altered MoRF Quantile\",\n#             np.arange(NBINS),lgnd=True)\n\n    makePlot(suffDf.groupby(\"MutPred2ScoreBin\")[f\"numD{d}\"],ax[0],\"MutPred2 Score Quantile\",\n            np.arange(NBINS),\"upper right\",byCount=True,yLab=True)\n    \n    makePlot(suffDf.groupby(\"MutPred2PaperBin\")[f\"numD{d}\"],ax[1],\"MutPred2 Score ClinGen SVI WG Recommendation\",\n            [\"B3\",\"B2\",\"B1\",\"-\",\"P1\",\"P2\",\"P3\"],byCount=True)\n    \n    makePlot(suffDf.groupby(\"alteredResidueBin\")[f\"numD{d}\"],ax[2],\"Altered PPI Residue Quantile\",\n            np.arange(NBINS),byCount=True)\n    \n    makePlot(suffDf.groupby(\"alteredHotspotBin\")[f\"numD{d}\"],ax[3],\"Altered PPI Hotspot Quantile\",\n            np.arange(NBINS),byCount=True)\n    \n    makePlot(suffDf.groupby(\"alteredMoRFBin\")[f\"numD{d}\"],ax[4],\"Altered MoRF Quantile\",\n            np.arange(NBINS),byCount=True,lgnd=True)\n    if GAIN_OR_LOSS:\n        fig.suptitle(f\"Level {d+1} - Number of PPIs Affected |WT-MT| >= 2\")\n    else:\n        fig.suptitle(f\"Level {d+1} - Number of PPIs Affected (WT-MT) >= 2\")\n#     plt.savefig(f\"figs/mutPredCorrelation_lvl_{d+1}.pdf\",format=\"pdf\")\n#     plt.savefig(f\"figs/mutPredCorrelation_lvl_{d+1}.jpg\",format=\"jpg\")\n    plt.show()\n\n\nfrom sklearn.metrics import roc_auc_score\n\n\nroc_auc_score((suffDf.numD2 > 0).values,\n              suffDf[\"MutPred2 score\"].values)\n\n\nroc_auc_score((suffDf.numD2 > 0).values,\n              suffDf[\"AlteredPPIResiduePr\"].values)\n\n\nroc_auc_score((suffDf.numD2 > 0).values,\n              suffDf[\"AlteredPPIHotspotPr\"].values)\n\n\nroc_auc_score((suffDf.numD2 > 0).values,\n              suffDf[\"AlteredMoRFPr\"].values)\n\n\nsuffDf"
  },
  {
    "objectID": "11_Format_for_Rosetta.html",
    "href": "11_Format_for_Rosetta.html",
    "title": "ppi",
    "section": "",
    "text": "import networkx as nx\n\n\n\nedgotype_train = nx.read_gpickle(\"data/y2hEdgotyping/edgotype_train.gpickle\")\n\n\nnext(iter(edgotype_train.edges(data=True)))\n\n\nedgotype_train.nodes[\"ENSG00000155755\"]\n\n\nedgotype_train.nodes[\"ENSG00000134049\"][\"seq\"][:77] + \"P\" + edgotype_train.nodes[\"ENSG00000134049\"][\"seq\"][78:]\n\n\nedgotype_train.nodes[\"ENSG00000134049\"][\"seq\"]\n\n\nn2e = {ensg: {\"structure\":edgotype_train.nodes[ensg][\"alphafoldStructures\"],\n              \"mutations\":[]} for ensg in edgotype_train.nodes()}\n\n\nfor u,v,e in edgotype_train.edges(data=True):\n    k = e[\"db_ensembl_gene_id_mt\"]\n    n2e[k][\"mutations\"].append(e[\"Substitution\"])\n\n\nn2e = {k:{\"mutations\":v[\"mutations\"],\n         \"structure\":v[\"structure\"][0]} for k,v in n2e.items() if len(v[\"mutations\"]) and len(v[\"structure\"])}\n\n\nfrom Bio.PDB import PDBParser\n\n\nparser = PDBParser()\n\n\nimport gzip\n\n\nfor k,v in n2e.items():\n    with gzip.open(v[\"structure\"],\"rt\") as f:\n        struc =parser.get_structure(\"AF-O75800\", f)\n        n2e[k][\"chains\"] = [c.id for c in struc.get_chains()][0]\n\n\nmutfile = lambda loc,chain,aa_mut:f\"\"\"NATRO\nstart\n{loc} {chain} PIKAA {aa_mut}\n\"\"\"\n\n\nimport os\n\n\nfrom shutil import move,copy\n\n\nimport sys\n\n\nimport subprocess\n\n\nfrom tqdm import tqdm\n\n\nfor k,v in tqdm(n2e.items(),total=len(n2e)):\n    for mutation in set(v[\"mutations\"]):\n        pth = f\"/home/dzeiberg/flex_ddG_tutorial/ppi_inputs/{k}_{mutation[:-1]}_{mutation[-1]}\"\n        if not os.path.isdir(pth):\n            os.mkdir(pth)\n        with open(os.path.join(pth,\"chains_to_move.txt\"),\"w\") as f:\n            f.write(v[\"chains\"])\n#         copy(v[\"structure\"],pth)\n        sf = v[\"structure\"]\n        of = os.path.join(pth,f\"{k}.pdb\")\n        os.system(f\"gunzip -c {sf} > {of}\")\n        os.chdir(pth)\n        os.system(f\"/data/utilities/bio/rosetta/rosetta.source.release-333/main/tools/protein_tools/scripts/clean_pdb.py {of} A\")\n        os.system(f\"rm {of}\")\n        with open(os.path.join(pth,\"nataa_mutations.resfile\"),\"w\") as f:\n            f.write(mutfile(mutation[1:-1],v[\"chains\"],mutation[-1]))"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nfoo\n\n foo ()"
  },
  {
    "objectID": "08_InterproScan_Results.html",
    "href": "08_InterproScan_Results.html",
    "title": "ppi",
    "section": "",
    "text": "import pandas as pd\n\n\n\ncolnames = [f[:f.find(\"(e.g.\")].replace(\"(\",\"\").strip() for f in \"\"\"Protein accession (e.g. P51587)\nSequence MD5 digest (e.g. 14086411a2cdf1c4cba63020e1622579)\nSequence length (e.g. 3418)\nAnalysis (e.g. Pfam / PRINTS / Gene3D)\nSignature accession (e.g. PF09103 / G3DSA:2.40.50.140)\nSignature description (e.g. BRCA2 repeat profile)\nStart location (e.g. 0)\nStop location (e.g. 1)\nScore (e.g. 3.1E-52)\nStatus(e.g. T: true)\nDate (e.g.)\nInterPro annotations Accession (e.g. IPR002093)\nInterPro annotations Description (e.g. BRCA2 repeat)\nGO annotations (e.g. GO:0005515) - optional column; only displayed if –goterms option is switched on)\nPathways annotations (e.g. REACT_71) - optional column; only displayed if –pathways option is switched on)\"\"\".split(\"\\n\")]\n\n\ninterpro = pd.read_csv(\"data/y2hEdgotyping/interproScan/sequences.fasta.tsv\",delimiter=\"\\t\",\n            header=None,names= colnames)\n\n\ninterpro.head()\n\n\ninterpro[~interpro[\"InterPro annotations Accession\"].isna()].Analysis.value_counts()\n\n\ninterpro[\"InterPro annotations Accession\"].value_counts()\n\n\ninterpro.dropna(subset=[\"InterPro annotations Accession\"])\n\n\ninterpro[\"InterPro annotations Description\"].value_counts()\n\n\ninterpro[\"InterPro annotations Description\"].dropna().unique()\n\n\n[d for d in interpro[\"InterPro annotations Description\"].dropna().unique() if \"binding\" in d]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ppi",
    "section": "",
    "text": "pip install ppi"
  },
  {
    "objectID": "09_Interactome_Insider_Binding_Residues.html",
    "href": "09_Interactome_Insider_Binding_Residues.html",
    "title": "ppi",
    "section": "",
    "text": "import networkx as nx\n\n\n\nedgotype_train = nx.read_gpickle(\"data/y2hEdgotyping/edgotype_train.gpickle\")\n\n\nimport pandas as pd\n\n\nintinsider = pd.read_csv(\"/data/dzeiberg/interactomeInsider/H_sapiens_interfacesHQ.txt\",\n                         delimiter=\"\\t\")\n\n\nintinsider\n\n\nuniprotEdges = []\nfor id_x, id_y,edge in edgotype_train.edges(data=True):\n    x = edgotype_train.nodes[id_x]\n    y = edgotype_train.nodes[id_y]\n    acc_x = x[\"uniprotMatches\"].Entry.values\n    acc_y = y[\"uniprotMatches\"].Entry.values\n    uniprotEdges.append((acc_x,acc_y,id_x,id_y,edge))\n\nuniprotEdges = pd.DataFrame([pd.Series(data=[e[0][0] if len(e[0]) else np.nan for e in uniprotEdges],name=\"P1\"),\n                             pd.Series(data=[e[1][0] if len(e[1]) else np.nan for e in uniprotEdges],name=\"P2\"),\n                             pd.Series(data=[e[2] for e in uniprotEdges],name=\"ensg_P1\"),\n                             pd.Series(data=[e[3] for e in uniprotEdges],name=\"ensg_P2\"),\n                             pd.Series(data=[e[4] for e in uniprotEdges],name=\"edge_attrs\")]).T\n\n\nuniprotEdges\n\n\nedgePredictedBindingSites = pd.concat((pd.merge(intinsider,uniprotEdges,how=\"inner\",\n                                                left_on=[\"P1\",\"P2\"],\n                                                right_on=[\"P1\",\"P2\"]),\n                                       pd.merge(intinsider,uniprotEdges,how=\"inner\",\n                                                left_on=[\"P1\",\"P2\"],\n                                                right_on=[\"P2\",\"P1\"]).drop([\"P1_y\",\n                                                                            \"P2_y\"],\n                                                                           axis=1).rename({\"P1_x\":\"P1\",\n                                                                                                           \"P2_x\":\"P2\"},axis=1))).reset_index()\n\n\ndef parseRanges(s):\n    if \",\" not in s:\n        return []\n    vals = s[1:-1].split(\",\")\n    values = []\n    if not len(vals):\n        return values\n    for v in vals:\n        if \"-\" in v:\n            v0, v1 = [int(i) for i in v.split(\"-\")]\n            values += range(v0,v1+1)\n        else:\n            try:\n                values.append(int(v))\n            except ValueError:\n                print(s)\n                raise ValueError\n    return values\n\n\nedgePredictedBindingSites = edgePredictedBindingSites.assign(P1_IRES=edgePredictedBindingSites.P1_IRES.apply(parseRanges),\n                                                             P2_IRES=edgePredictedBindingSites.P2_IRES.apply(parseRanges))\n\n\ndef make_y2h_target(d):\n    names = [\"LWH1_f_\",\"LWH10_f_\",\"LWH25_f_\",\n             \"LWA_f_\",\"LWAH1_f_\"]\n    deltas = np.zeros(len(names))\n    for i,name in enumerate(names):\n        deltas[i] = d[name+\"wt\"] - d[name+\"mt\"]\n    return np.any(deltas >= 2)\n\n\ndef proximityToSite(r):\n    loc = int(r[\"edge_attrs\"][\"aa_change_mt\"][3:-3])\n    if r[\"edge_attrs\"][\"db_ensembl_gene_id_mt\"] == r.ensg_P1:\n        site = r.P1_IRES\n    else:\n        site = r.P2_IRES\n    if not len(site):\n        return -1e10\n    return -1 * min([abs(loc - s) for s in site])\n\n\nedgePredictedBindingSites = edgePredictedBindingSites.assign(proximity_to_site=edgePredictedBindingSites.apply(proximityToSite,axis=1))\n\n\nedgePredictedBindingSites = edgePredictedBindingSites.assign(y2h_target=edgePredictedBindingSites.edge_attrs.apply(make_y2h_target),\n                                                             variantAtBindingSite=edgePredictedBindingSites.apply(lambda row:row[\"edge_attrs\"],axis=0))\n\n\nfrom sklearn.metrics import roc_auc_score\n\n\nedgePredictedBindingSites\n\n\nroc_auc_score(edgePredictedBindingSites[\"y2h_target\"],edgePredictedBindingSites[\"variant_at_site\"])"
  },
  {
    "objectID": "add_uniprot_search_results_to_edgotype_graph.html",
    "href": "add_uniprot_search_results_to_edgotype_graph.html",
    "title": "Analyze Hit Rate of Edgotype Nodes with Uniprot",
    "section": "",
    "text": "import pandas as pd"
  },
  {
    "objectID": "add_uniprot_search_results_to_edgotype_graph.html#train-uniprot-matches",
    "href": "add_uniprot_search_results_to_edgotype_graph.html#train-uniprot-matches",
    "title": "Analyze Hit Rate of Edgotype Nodes with Uniprot",
    "section": "Train Uniprot Matches",
    "text": "Train Uniprot Matches\n\nnp.unique([n[1][\"uniprotMatches\"].shape[0] for n in edgotype_m.nodes(data=True)],\n          return_counts=True)"
  },
  {
    "objectID": "add_uniprot_search_results_to_edgotype_graph.html#train-alphafold-matches",
    "href": "add_uniprot_search_results_to_edgotype_graph.html#train-alphafold-matches",
    "title": "Analyze Hit Rate of Edgotype Nodes with Uniprot",
    "section": "Train Alphafold Matches",
    "text": "Train Alphafold Matches\n\nnp.unique([len(n[1][\"alphafoldStructures\"]) for n in edgotype_m.nodes(data=True)],\n          return_counts=True)\n\n\nnext(iter(edgotype_m.edges(data=True)))\n\n\nnext(iter(edgotype_m.nodes(data=True)))[1][\"uniprotMatches\"]\n\n\nnx.write_gexf(edgotype_m,\"data/y2hEdgotyping/edgotype_x_uniprot.gefx\")"
  },
  {
    "objectID": "16_Visualize_Scores.html",
    "href": "16_Visualize_Scores.html",
    "title": "ppi",
    "section": "",
    "text": "import networkx as nx\n\nedgotype = nx.read_gexf(\"data/y2hEdgotyping/edgotype.gefx\")\n\n\n\nimport pandas as pd\nimport numpy as np\nimport itertools\n\n\n# CENTERS = [\"ENSG00000004838\",\"ENSG00000134371\"]\nsubgraph = edgotype\n# subgraph = edgotype.subgraph(CENTERS+list(itertools.chain.from_iterable([edgotype.neighbors(c) for c in CENTERS])))\n\n\nfrom collections import Counter\n\n\nweightedEdges = [tuple([*k,v]) for k,v in dict(Counter(subgraph.edges())).items()]\n\n\nund_g = nx.Graph()\n\nund_g.add_weighted_edges_from(weightedEdges)\n\n\npos = nx.drawing.nx_agraph.graphviz_layout(und_g,prog='twopi')\n\n\n# Step 2: Convert graph data from NetworkX's format to the pandas DataFrames expected by Altair\n\npos_df = pd.DataFrame.from_records(dict(node_id=k,\n                                        x=x,\n                                        y=y) for k,(x,y) in pos.items())\n\n\npos_df\n\n\nnode_df = pd.DataFrame.from_records(dict(data,\n                                         **{'node_id': n,\n                                           'db': n}) for n,data in subgraph.nodes.data())\n\n\nnode_df\n\n\nimport itertools\n\n\nedge_data = ((dict(d, **{\"edge_id\": i,\n                         'end':\"source\",\n                         \"node_id\":s}),\n             dict(d, **{\"edge_id\": i,\n                        \"end\": 'target',\n                        'node_id': t})) for i,\n             (s,t,d) in enumerate(und_g.edges(data=True)))\nedge_df = pd.DataFrame.from_records(itertools.chain.from_iterable(edge_data))\n\n\nedge_df\n\n\ndata = pd.DataFrame.from_records([{\"db\":e[\"db_ensembl_gene_id_mt\"],\n                               \"ad\": e[\"ad_ensembl_gene_id_mt\"],\n                               \"mt\": e[\"aa_change_mt\"],\n                               \"control_wt\": e[\"LW_wt\"],\n                               \"control_mt\": e[\"LW_mt\"],\n                               \"level_1_wt\": e[\"LWH1_f_wt\"],\n                               \"level_1_mt\": e[\"LWH1_f_mt\"],\n                               \"level_2_wt\": e[\"LWH10_f_wt\"],\n                               \"level_2_mt\": e[\"LWH10_f_mt\"],\n                               \"level_3_wt\": e[\"LWH25_f_wt\"],\n                               \"level_3_mt\": e[\"LWH25_f_mt\"],\n                               \"level_4_wt\": e[\"LWA_f_wt\"],\n                               \"level_4_mt\": e[\"LWA_f_mt\"],\n                               \"level_5_wt\": e[\"LWAH1_f_wt\"],\n                               \"level_5_mt\": e[\"LWAH1_f_mt\"],\n                                  \"db_sym\": subgraph.nodes[e[\"db_ensembl_gene_id_mt\"]][\"symbol\"],\n                                  \"ad_sym\": subgraph.nodes[e[\"ad_ensembl_gene_id_mt\"]][\"symbol\"],} for _,_,e in subgraph.edges(data=True)])\ndata[\"name\"] = data[['db_sym', 'mt', 'ad_sym']].agg('-'.join, axis=1)\n\n\nimport matplotlib.pyplot as plt\n\n\ndata.dropna()[[\"db\",\"ad\"]].drop_duplicates()\n\n\ndata.shape\n\n\nwtSub = data[[\"db\",\"ad\",\"mt\"]+[f\"level_{i}_wt\" for i in range(1,6)]].drop_duplicates(subset=[\"db\",\"ad\",\"mt\"])\n\n\nwtSub[wtSub.isna().any(1)]\n\n\ndata[data[[c for c in data.columns if \"_mt\" in c]].isna().any(1)]\n\n\ndata.dropna(subset=[c for c in data.columns if \"_mt\" not in c])\n\n\ncounts = pd.DataFrame(data[[\"db\",\"ad\"]+[f\"level_{i}_wt\" for i in range(1,6)]].drop_duplicates(subset=[\"db\",\"ad\"])[[f\"level_{i}_wt\" for i in range(1,6)]].dropna().value_counts())\n\n\ncounts\n\n\ncounts.sort_values(by=\"level_1_wt\",ascending=False)\n\n\nimport altair as alt\nfrom altair import expr, datum\nalt.data_transformers.disable_max_rows()\n\n\nbrush = alt.selection_single(fields=['db'])\n# point_sel = alt.selection_multi(on=\"[mousedown[event.ctrlKey], mouseup] > mousemove\")\n\n\nx,y = alt.X('x:Q', axis=None), alt.Y('y:Q', axis=None)\n# use a lookup to tie position data to the other graph data\nnode_position_lookup = {\n    'lookup': 'node_id', \n    'from_': alt.LookupData(data=pos_df, key='node_id', fields=['x', 'y'])\n}\nnodes = (\n    alt.Chart(node_df)\n    .mark_circle(size=10, opacity=1)\n    .encode(x=x, y=y, tooltip='symbol:N',color=alt.condition(brush,alt.value('blue'),alt.value('lightgray')))\n    .transform_lookup(**node_position_lookup).add_selection(brush)\n)\nedges = (\n    alt.Chart(edge_df)\n    .mark_line(color='gray')\n    .encode(x=x, y=y, detail='edge_id:N',size=\"weight:Q\")  # `detail` gives one line per edge\n    .transform_lookup(**node_position_lookup)\n)\ngraphChart = (\n    (edges+nodes)).interactive()\n#     \n# )\n\n\nchart = alt.Chart(data).mark_point(filled=True).encode(color=\"mt\",\n                                            opacity=alt.condition(brush,alt.value(1.0),alt.value(0.0025)),\n                                            tooltip=alt.condition(brush,\"name:N\",alt.value('')),\n                                            shape='ad_sym').add_selection(brush)\nc1 = chart.encode(x=alt.X('level_1_wt:Q',\n                          scale=alt.Scale(domain=[-1,5])),\n                  y=alt.Y('level_1_mt:Q',\n                          scale=alt.Scale(domain=[-1,5])),).transform_calculate(level_1_wt='datum.level_1_wt + sampleNormal(0,.05)',\n                                                             level_1_mt='datum.level_1_mt + sampleNormal(0,.05)').interactive()\nc2 = chart.encode(x=alt.X('level_2_wt:Q',scale=alt.Scale(domain=[-1,5])),\n                  y=alt.Y('level_2_mt:Q',scale=alt.Scale(domain=[-1,5]))).transform_calculate(level_2_wt='datum.level_2_wt + sampleNormal(0,.05)',\n                                                                                            level_2_mt='datum.level_2_mt + sampleNormal(0,.05)').interactive()\n\nc3 = chart.encode(x=alt.X('level_3_wt:Q',scale=alt.Scale(domain=[-1,5])),\n                  y=alt.Y('level_3_mt:Q',scale=alt.Scale(domain=[-1,5]))).transform_calculate(level_3_wt='datum.level_3_wt + sampleNormal(0,.05)',\n                                                                                            level_3_mt='datum.level_3_mt + sampleNormal(0,.05)').interactive()\n\nc4 = chart.encode(x=alt.X('level_4_wt:Q',scale=alt.Scale(domain=[-1,5])),\n            y=alt.Y('level_4_mt:Q',scale=alt.Scale(domain=[-1,5]))).transform_calculate(level_4_wt='datum.level_4_wt + sampleNormal(0,.05)',\n                                                             level_4_mt='datum.level_4_mt + sampleNormal(0,.05)').interactive()\n\nc5 = chart.encode(x=alt.X('level_5_wt:Q',scale=alt.Scale(domain=[-1,5])),\n            y=alt.Y('level_5_mt:Q',scale=alt.Scale(domain=[-1,5]))).transform_calculate(level_5_wt='datum.level_5_wt + sampleNormal(0,.05)',\n                                                             level_5_mt='datum.level_5_mt + sampleNormal(0,.05)').interactive()\n\n\ndata.head()\n\n\ndata.shape\n\n\nc = alt.vconcat(alt.hconcat(c1,c2),\n                alt.hconcat(c3,c4),\n                alt.hconcat(c5,graphChart)).configure_view(strokeWidth=0)\n\nlevel_1 : LWH1_f - LWH1_f : Selective media score to test for interaction, for yeast spotted on SC -LW -histidine +1 mM 3AT\nlevel_2 : LWH10_f - LWH10_f : Selective media score to test for interaction, for yeast spotted on SC -LW -histidine +10 mM 3AT\nlevel_3 : LWH25_f - LWH25_f : Selective media score to test for interaction, for yeast spotted on SC -LW -histidine +25 mM 3AT\nlevel_4 : LWA_f - LWA_f :Selective media score to test for interaction, for yeast spotted on SC -LW -adenine\nlevel_5 : LWAH1_f - LWAH1_f :Selective media score to test for interaction, for yeast spotted on SC -LW -adenine -histidine +1 mM 3AT\n\n\nc\n\n\ndata2 = data.assign(delta_1=data.level_1_wt - data.level_1_mt,\n            delta_2=data.level_2_wt - data.level_2_mt,\n            delta_3=data.level_3_wt - data.level_3_mt,\n            delta_4=data.level_4_wt - data.level_4_mt,\n            delta_5=data.level_5_wt - data.level_5_mt)\n\n\ndX = data2[[c for c in data2.columns if \"delta\" in c]].dropna().values\n\n# dX = dX[dX]\n\n\ndX\n\n\nplt.plot(range(1,6),dX.mean(0))\nplt.fill_between(range(1,6),\n                  dX.mean(0)-1.96*dX.std(0),\n                 dX.mean(0)+1.96*dX.std(0),alpha=.25)\n\n\ndX.shape\n\n\ndX = dX[~((dX < 0).any(1))]\n\n\ndX.shape\n\n\nfrom sklearn.decomposition import PCA\n\n\npca = PCA(n_components=1)\n\n\npca.fit(dX)\n\n\nplt.hist(pca.transform(dX),bins=25)\n\n\npca.components_\n\n\ndX"
  },
  {
    "objectID": "15_Selective_Media_Score_Distribution.html",
    "href": "15_Selective_Media_Score_Distribution.html",
    "title": "ppi",
    "section": "",
    "text": "import networkx as nx\n\nedgotype_train = nx.read_gpickle(\"data/y2hEdgotyping/edgotype_train.gpickle\")\nedgotype_val = nx.read_gpickle(\"data/y2hEdgotyping/edgotype_val.gpickle\")\nedgotype_test = nx.read_gpickle(\"data/y2hEdgotyping/edgotype_test.gpickle\")\n\n\n\ndef getPPILabel(e):\n    y = np.zeros(6)\n    for i,med in enumerate([\"LW_\",\"LWH1_f_\", \"LWH10_f_\",\n               \"LWH25_f_\", \"LWA_f_\",\"LWAH1_f_\"]):\n        s_wt = e[med+\"wt\"]\n        s_mt = e[med+\"mt\"]\n        if not (np.isnan(s_wt) or np.isnan(s_mt)):\n            y[i] = s_wt - s_mt\n    return y\n\n\nmut2Edge = {}\nfor e in list(edgotype_train.edges(data=True)) + list(edgotype_val.edges(data=True)) + list(edgotype_test.edges(data=True)):\n    s = e[2][\"Substitution\"]\n    if s not in mut2Edge:\n        mut2Edge[s] = []\n    mut2Edge[s].append((*e,getPPILabel(e[2])))\n\n\nvals = []\nfor mut,edges in mut2Edge.items():\n    cln_sig = set([e[2][\"clinical_significance_mt\"] for e in edges])\n    scores = [e[-1] for e in edges]\n    vals.append({\"mutation\":mut, \"cln_sig\":cln_sig,\"scores\":scores})\n\n\nb = []\np = []\nvus = []\nfor v in vals:\n    avgScore = np.stack(v[\"scores\"]).mean(0)\n    if \"Pathogenic\" in v[\"cln_sig\"]:\n        p.append(avgScore)\n    elif \"Benign\" in v[\"cln_sig\"]:\n        b.append(avgScore)\n    else:\n        vus.append(avgScore)\n\n\nbV = np.stack(b)\n\n\npV = np.stack(p)\n\n\nvusV = np.stack(vus)\n\n\nimport matplotlib.pyplot as plt\n\n\nbins\n\n\nfig,ax = plt.subplots(4,6,figsize=(16,6),sharex=True)\nfor i in range(bV.shape[1]):\n    bVals,bins,_ = ax[0,i].hist(bV[:,i],bins=np.arange(-1,4),density=True)\n    vusVals,_,_ = ax[1,i].hist(vusV[:,i],bins=bins,density=True)\n    pVals,_,_ = ax[2,i].hist(pV[:,i],bins=bins,density=True)\n    ax[3,i].plot(bins[:-1],pVals/bVals)"
  },
  {
    "objectID": "data_exploration.html",
    "href": "data_exploration.html",
    "title": "Vidal Lab Data",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n\naaPairs = {\"A\":\"Ala\", \"R\":\"Arg\", \"N\":\"Asn\", \"D\":\"Asp\",\n           \"C\":\"Cys\", \"E\":\"Glu\", \"Q\":\"Gln\", \"G\":\"Gly\",\n           \"H\":\"His\", \"I\":\"Ile\", \"L\":\"Leu\", \"K\":\"Lys\",\n           \"M\":\"Met\", \"F\":\"Phe\", \"P\":\"Pro\", \"S\":\"Ser\",\n           \"T\":\"Thr\", \"W\":\"Trp\", \"Y\":\"Tyr\", \"V\":\"Val\"}\n\naaTable = dict(list(zip(*list(zip(*aaPairs.items()))[::-1])))\n\n\nedgotypes = pd.read_csv(\"data/y2hEdgotyping/qY2H_edgotyping_data.csv\",index_col=0)\n\n\nad_orf_id : ID of the ORF fused on Activation Domain (AD)\ndb_orf_id : ID of the ORF fused on DNA-binding Domain (DB)\ndb_mut_id : ID of the variant\nstandard_batch : Name of the experiment\nassay_id : Type of Y2H experiment, I can explain you the details of that in our next meeting\nLW : Selective media score to control for the presence of both plasmids (AD and DB); scores range from 0-4 and should 3+ for most\nLWH1_f : Selective media score to test for interaction, for yeast spotted on SC -LW -histidine +1 mM 3AT (“level 1”); scores range from 0-4\nLWH10_f : Selective media score to test for interaction, for yeast spotted on SC -LW -histidine +10 mM 3AT (“level 2”); scores range from 0-4\nLWH25_f : Selective media score to test for interaction, for yeast spotted on SC -LW -histidine +25 mM 3AT (“level 3”); scores range from 0-4\nLWA_f :Selective media score to test for interaction, for yeast spotted on SC -LW -adenine (“level 4”); scores range from 0-4\nLWAH1_f :Selective media score to test for interaction, for yeast spotted on SC -LW -adenine -histidine +1 mM 3AT (“level 5”); scores range from 0-4\nn_condition : number of valid conditions (some conditions might be ignored if contamination for instance)\nscore : sum positive levels; a level is considered positive if score > 1\nscore_norm : score/n_condition\nad_symbol : HGNC symbol of the ORF fused on Activation Domain (AD)\nad_ensembl_gene_id : ad_ensembl_gene_id of the ORF fused on Activation Domain (AD)\ndb_symbol : HGNC symbol of the ORF fused on the DNA-binding Domain (DB)\ndb_ensembl_gene_id : ad_ensembl_gene_id of the ORF fused on the DNA-binding Domain (DB)\nnt_change : nt_change\naa_change : aa_change\nclinical_significance : clinical_significance from ClinVar\nallele_score : score_norm\nwt_score : score_norm of the respective wild-type (WT score_norm if the row is WT)\n\n\nmtSub = edgotypes[edgotypes.clinical_significance != \"WT\"]\nwtSub = edgotypes[edgotypes.clinical_significance == \"WT\"]\n\n\nmtSub[[c for c in mtSub.columns if \"LW\" in c] + [\"score\"]]\n\n\nwtTraj = wtSub[[c for c in wtSub.columns if \"LW\" in c]].dropna().values\n\n\nmtTraj = mtSub[[c for c in mtSub.columns if \"LW\" in c]].dropna().values\n\n\nplt.fill_between(range(6),wtTraj.mean(0) - 1.96 * wtTraj.std(0),\n                wtTraj.mean(0) + 1.96* wtTraj.std(0),color=\"blue\")\nplt.plot(wtTraj.mean(0),color=\"blue\",label=\"WT\")\n\nplt.fill_between(range(6),mtTraj.mean(0) - 1.96 * mtTraj.std(0),\n                mtTraj.mean(0) + 1.96* mtTraj.std(0),alpha=.5,color=\"orange\")\nplt.plot(mtTraj.mean(0),color=\"orange\",label=\"MT\")\nplt.legend()\n\n\nmergedEdgoTypes = pd.merge(mtSub,wtSub,how=\"left\",\n                           left_on=[\"db_orf_id\",\"ad_orf_id\"],\n                           right_on=[\"db_orf_id\",\"ad_orf_id\"],\n                           suffixes=[\"_mt\",\"_wt\"])\n\n\norf_seqs = pd.read_csv(\"data/y2hEdgotyping/ORF_sequence.csv\",index_col=0)\n\n\ny2HMerged = pd.merge(pd.merge(mergedEdgoTypes,orf_seqs,left_on=\"db_orf_id\",\n         right_on=\"orf_id\",how=\"left\"),orf_seqs,left_on=\"ad_orf_id\",\n        right_on=\"orf_id\",suffixes=[\"_db\",\"_ad\"])\n\n\ny2HMerged\n\n\ndef validateSeqs(r):\n    ref,loc,alt = aaTable[r.aa_change_mt[:3]],int(r.aa_change_mt[3:-3]),aaTable[r.aa_change_mt[-3:]]\n    return (r.p_seq_db[loc-1] == ref) and (ref != alt)\n\n\ny2HMerged[y2HMerged.apply(validateSeqs,axis=1)]\n\n\ny2HMerged[y2HMerged.apply(validateSeqs,axis=1)].to_csv(\"data/y2H_edgotyping_10_7_22/y2HMerged.csv\")\n\n\nLook into Overlap with MaveDB\n\ny2HSymbols = set(y2HMerged.db_symbol_mt).union(set(y2HMerged.ad_symbol_mt))\n\n\nimport requests\n\n\nr = requests.get(\"https://www.mavedb.org/api/target/\")\n\n\nmaveSymbols = set([t[\"name\"] for t in r.json()])\n\n\nmaveSymbols.intersection(y2HSymbols)\n\n\nmaveIntersection = y2HMerged[(y2HMerged.db_symbol_mt.isin(maveSymbols))]\n\n\nmaveIntersection[[\"db_mut_id_mt\",\"db_orf_id\"]].drop_duplicates()\n\n\nmaveIntersection = maveIntersection.assign(hgvs_pro=maveIntersection.aa_change_mt.apply(lambda s: \"p.\"+s))\n\n\nmaveIntersection.db_symbol_mt.value_counts()\n\n\nmaves = [pd.read_csv(\"data/maveDB/urn_mavedb_00000096-a-1_scores_GATK.csv\",header=4),\n        pd.read_csv(\"data/maveDB/urn_mavedb_00000001-d-1_scores_TPK1.csv\",header=4)]\n\n\nmaves[1]\n\n\nmaveIntersectionJoined = pd.merge(maveIntersection,maves[0],left_on=\"hgvs_pro\",right_on=\"hgvs_pro\")\n\n\nmaveIntersectionJoined.score\n\n\nplt.scatter(maveIntersectionJoined.score, maveIntersectionJoined.LWH25_f_wt - maveIntersectionJoined.LWH25_f_mt)\nplt.xlabel(\"Mave\")\nplt.ylabel(r\"$\\Delta$ PPI (WT-MT)\")\n# plt.yticks(ticks=[0,1,2],labels=list(\"012\"))\n\n\nimport seaborn as sns\n\nimport scipy.stats as ss\n\nfig,ax = plt.subplots(2,6,figsize=(24,12))\nfor lvl,(axi,mtScores,wtScores) in enumerate(zip(ax.T,[y2HMerged.LW_mt,\n                                          y2HMerged.LWH1_f_mt,\n                                          y2HMerged.LWH10_f_mt,\n                                          y2HMerged.LWH25_f_mt,\n                                          y2HMerged.LWA_f_mt,\n                                         y2HMerged.LWAH1_f_mt],\n                                              [y2HMerged.LW_wt,\n                                          y2HMerged.LWH1_f_wt,\n                                          y2HMerged.LWH10_f_wt,\n                                          y2HMerged.LWH25_f_wt,\n                                          y2HMerged.LWA_f_wt,\n                                         y2HMerged.LWAH1_f_wt]),start=0):\n    mask = ~(pd.isna(mtScores) | pd.isna(wtScores))\n\n    _,xb,yb,binVal = ss.binned_statistic_2d(mtScores[mask],wtScores[mask],np.zeros(mask.sum()),statistic=np.sum,\n                                          bins=[np.arange(6),np.arange(6)],expand_binnumbers=True,)\n    binVal -= 1\n    cnts = np.zeros((5,5))\n    for b in binVal.T:\n        cnts[b[0],b[1]] += 1\n    cnts /= cnts.sum()\n    cnts *= 100\n    sns.heatmap(cnts,ax=axi[0],annot=True,cmap=\"rocket_r\",vmin=0,vmax=100,cbar=lvl == ax.shape[1]-1)\n    axi[0].set_xlabel(\"WT Score\")\n    if lvl == 0:\n        axi[0].set_ylabel(\"MT Score\")\n    axi[1].hist(mtScores.dropna(),bins=np.arange(0,6),label=\"mt\")\n    axi[1].hist(wtScores.dropna(),bins=np.arange(0,6),alpha=.5,label=\"wt\")\n    axi[1].set_xlabel(\"Score\")\n    if lvl == 0:\n        axi[1].set_ylabel(\"Count\")\n    \n    if not lvl:\n        axi[0].set_title(f\"Control\")\n    else:\n        axi[0].set_title(f\"Level {lvl:d}\")\naxi[1].legend()\n\n\nfig.savefig(\"data/y2H_edgotyping_10_7_22/figs/scoreChanges.pdf\",format=\"pdf\")\n\n\ntot = 0\nwith open(\"data/y2hEdgotyping/mutpred2Results/variants.faa\",\"w\") as f:\n    for seq,group in y2HMerged[y2HMerged.apply(validateSeqs,axis=1)].groupby(\"p_seq_db\"):\n        orf_id = str(group.db_orf_id.unique()[0]).replace(\" \",\"\").replace(\";\",\"\").replace(\",\",\"\")\n        uniqueVariants = group.aa_change_mt.unique()\n        tot += len(uniqueVariants)\n        for v in uniqueVariants:\n            try:\n                int(v[3:-3])\n            except ValueError:\n                print(v)\n                raise ValueError\n        varstr = \" \".join([aaTable[v[:3]]+v[3:-3]+aaTable[v[-3:]] for v in uniqueVariants])\n        r = f\">db_orf_{orf_id} {varstr}\\n{seq}\\n\"\n        print(r)\n        f.write(r)\n\n\ntot\n\n\n\nNature Paper\n\nls data/natureExtensiveDisruption/\n\n\npd.read_excel(\"data/natureExtensiveDisruption/41467_2019_11959_MOESM10_ESM.xlsx\")\n\n\n\nHuRI Data\n\nhuriT = pd.read_csv(\"/data/dzeiberg/ppi/HuRI.tsv\",delimiter=\"\\t\",header=None)\n\n\nhuriT.columns = [\"A\",\"B\"]\n\n\nhuriT\n\n\nhuri = pd.read_csv(\"/data/dzeiberg/ppi/HuRI.psi\",delimiter=\"\\t\",header=None)\n\n\nhuri.shape\n\n\nhuri.loc[0]\n\n\nhuri.loc[0,3]\n\n\nhuriT[huriT.A == \"ENSG00000130518\"]\n\n\nhuriT[huriT.A == \"ENSG00000160014\"]\n\n\nhuriT"
  },
  {
    "objectID": "stability_data.html",
    "href": "stability_data.html",
    "title": "Interactome Insider Overlap",
    "section": "",
    "text": "import pandas as pd\n\n\ndf = pd.read_csv(\"/data/dzeiberg/stability/skempi_v2.csv\",delimiter=\";\")\n\n\ndf\n\n\nskempi_pdb = set([v[:v.find(\"_\")] if \"_\" in v else v for v in df[\"#Pdb\"].values])\n\n\nlen(set(\",\".join(df[\"Mutation(s)_cleaned\"].values).split(\",\")))\n\n\nimport networkx as nx\n\nedgotype = nx.read_gexf(\"data/y2hEdgotyping/edgotype.gefx\")\n\n\nimport os\nimport pandas as pd\nseqFiles = [pd.read_csv(f\"data/y2hEdgotyping/uniprotScan/sequence_{i}.tsv\",delimiter=\"\\t\") for i in range(6)]\n\nuniprotMatches = pd.concat(seqFiles)\ndef mergeWithUniprot(graph):\n    for node in graph.nodes(data=True):\n        seq = node[1][\"seq\"]\n        up = uniprotMatches[(uniprotMatches.Sequence == seq) & \\\n                            (uniprotMatches.Reviewed == \"reviewed\") & \\\n                           (uniprotMatches.Organism == \"Homo sapiens (Human)\")]\n        graph.nodes[node[0]][\"uniprotMatches\"] = up\n        alphafoldStructures = []\n        for uniprot_id in graph.nodes[node[0]][\"uniprotMatches\"][\"Entry\"]:\n            fp = f\"/data/dzeiberg/alphafold/predictions/AF-{uniprot_id}-F1-model_v4.pdb.gz\"\n            if os.path.isfile(fp):\n                alphafoldStructures.append(fp)\n        graph.nodes[node[0]][\"alphafoldStructures\"] = alphafoldStructures\n    return graph\n\n\nedgotype_x = mergeWithUniprot(edgotype)\n\n\nimport itertools\n\n\nedgotype_pdb = set(\";\".join(pd.concat([n[1][\"uniprotMatches\"].PDB for n in edgotype_x.nodes(data=True)]).dropna()).split(\";\"))\n\n\nnodeInt = {}\nfor node_id, n in edgotype_x.nodes(data=True):\n    if len(n[\"uniprotMatches\"]):\n        pdbn = n[\"uniprotMatches\"][\"PDB\"].values[0]\n        if type(pdbn) is str:\n            pdb_n = set(pdbn.split(\";\"))\n            if len(skempi_pdb.intersection(pdb_n)):\n                subs = [e[\"Substitution\"] for _,_,e in edgotype_x.edges(node_id,\n                                                                     data=True)]\n                nodeInt[node_id] = {\"pdb\":pdb_n,\"subs\":subs}\n\n\nskempiSub = edgotype_x.subgraph(list(nodeInt.keys()) + list(itertools.chain.from_iterable([edgotype_x.neighbors(i) for i in nodeInt.keys()])))\n\n\nfor k,v in nodeInt.items():\n    print(k,len(v[\"pdb\"]),len(v[\"subs\"]))\n\n\nnodeInt[\"ENSG00000150337\"]\n\n\nii = pd.read_csv(\"/data/dzeiberg/interactomeInsider/H_sapiens_interfacesHQ.txt\",delimiter=\"\\t\")\n\n\nii.Source.unique()\n\n\nii = ii[ii.Source.isin([\"PDB\",\"I3D\"])]\n\n\nens2uni = {}\nfor ensg, n in edgotype_x.nodes(data=True):\n    ens2uni[ensg] = n[\"uniprotMatches\"].Entry.values\n\n\nuniEdges = []\nfor i,j in edgotype_x.edges():\n    ui = ens2uni[i]\n    uj = ens2uni[j]\n    if len(ui) and len(uj):\n        uniEdges.append([ui[0],uj[0],i,j])\n\n\nsu = set([tuple(r[:2]) for r in np.array(uniEdges)])\n\nsii = set([tuple(r) for r in ii[[\"P1\",\"P2\"]].values])\n\n\nsu.intersection(sii)\n\n\nsubgraphNodes = set.union(*[set(tuple(u[2:])) for u in uniEdges if tuple(u[:2]) in sii])\n\n\nii_subgraph = edgotype_x.subgraph(subgraphNodes)\n\n\nlen(ii_subgraph.edges())\n\n\nedgedf = []\nfor i,j,e in ii_subgraph.edges(data=True):\n    edgedf.append({\"Pi\":ens2uni[e[\"db_ensembl_gene_id_mt\"]][0],\n                   \"Pj\": ens2uni[e[\"ad_ensembl_gene_id_mt\"]][0],\n                   \"edge\": e})\n\n\nfor i in range(len(edgedf)):\n    pi = edgedf[i][\"Pi\"]\n    pj = edgedf[i][\"Pj\"]\n    iisub = ii[((ii.P1 == pi) & (ii.P2 == pj)) | ((ii.P1 == pj) & (ii.P2 == pi))]\n    if len(iisub):\n        edgedf[i][\"ii_matches\"] = iisub\n    else:\n        edgedf[i][\"ii_matches\"] = pd.DataFrame()\n\n\ninside = []\noutside = []\nfor e in edgedf:\n    loc = e[\"edge\"][\"aa_change_mt\"][3:-3]\n    if len(e[\"ii_matches\"]):\n        p1r = e[\"ii_matches\"].iloc[0][\"P1_IRES\"][1:-1].replace(\"-\",\",\").split(\",\")\n        p2r = e[\"ii_matches\"].iloc[0][\"P2_IRES\"][1:-1].replace(\"-\",\",\").split(\",\")\n#         print(loc,p1r,p2r)\n        if ((e[\"Pi\"] == e[\"ii_matches\"].iloc[0][\"P1\"] and loc in p1r) or \\\n                                 (e[\"Pi\"] == e[\"ii_matches\"].iloc[0][\"P2\"] and loc in p2r)):\n            inside.append(e)\n        else:\n            outside.append(e)\n\n\nlen(inside)\n\n\nlen(outside)\n\n\nFindings\n242 instances in the edgotype data have matches in Interactome Insider High Quality dataset\n21 of the 242 are comprised of a mutation inside db protein’s predicted interacting residue range\n\ndef calc_score(e):\n    score = 0\n    for med in [\"LWH1_f_\",\"LWH10_f_\", \"LWH25_f_\",\n                \"LWA_f_\",\"LWAH1_f_\"]:\n#         score += int(e[med+\"mt\"] + 2 <= e[med+\"wt\"])\n        score += e[med + \"wt\"] - e[med + \"mt\"]\n    return score\n\n\ninsideScores = [calc_score(e[\"edge\"]) for e in inside]\noutsideScores = [calc_score(e[\"edge\"]) for e in outside]\n\n\nimport matplotlib.pyplot as plt\n\n\nfig,ax = plt.subplots(1,2,sharey=True)\nax[0].hist(insideScores,density=True)\nax[0].set_title(\"At Predicted Binding Residue\")\nax[1].hist(outsideScores,density=True)\n_ = ax[1].set_title(\"Not at Predicted Binding Residue\")\n# ax[0].set_xticks(np.arange(0,6))\n# _ = ax[1].set_xticks(np.arange(0,6))\nax[0].set_xlim(-20,20)\nax[1].set_xlim(-20,20)"
  },
  {
    "objectID": "wt_go_terms_mutpred2_and_node2vec_baseline_experiments.html",
    "href": "wt_go_terms_mutpred2_and_node2vec_baseline_experiments.html",
    "title": "Import HuRI-Union Reference Interactome",
    "section": "",
    "text": "aaPairs = {\"A\":\"Ala\", \"R\":\"Arg\", \"N\":\"Asn\", \"D\":\"Asp\",\n           \"C\":\"Cys\", \"E\":\"Glu\", \"Q\":\"Gln\", \"G\":\"Gly\",\n           \"H\":\"His\", \"I\":\"Ile\", \"L\":\"Leu\", \"K\":\"Lys\",\n           \"M\":\"Met\", \"F\":\"Phe\", \"P\":\"Pro\", \"S\":\"Ser\",\n           \"T\":\"Thr\", \"W\":\"Trp\", \"Y\":\"Tyr\", \"V\":\"Val\"}\n\naaTable = dict(list(zip(*list(zip(*aaPairs.items()))[::-1])))\n\n\nimport pandas as pd\n\n\nhuri = pd.read_csv(\"data/HuRI/HI-union.tsv\",\n                   delimiter=\"\\t\",\n                   header=None)\nhuri.columns = [\"A\",\"B\"]\n\n\nImport Processed Dataset of Variant Effect on PPI measured by Y2H\n\ny2h = pd.read_csv(\"data/y2hEdgotyping/y2HMerged.csv\",index_col=0)\n\n\ny2h\n\n\ngene_ids = list(set(huri.A).union(set(huri.B)))\n\n\n\nSKIP - Get GO Terms\n\n# with open(\"data/HuRI/HI-union.ensg.csv\",\"w\") as f:\n#     f.write(\"\\n\".join(gene_ids))\n\n# import requests\n\n# len(gene_ids)\n\n# def batchiter(seq, size):\n#     return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n\n# from tqdm import tqdm\n\n# responses = []\n# for batch in tqdm(batchiter(gene_ids,100),total=len(gene_ids)//100):\n#     r = requests.post(\"https://mygene.info/v3/gene?fields=go\",data={\"ids\":\",\".join(batch)})\n#     if r.status_code == 200:\n#         terms = r.json()\n#         responses.append(terms)\n#     else:\n#         print(r.status_code)\n\nBP = Biological Process\nCC = Cellular Component\nMF = Molecular Function\n\ndef extract(terms):\n    if terms is None:\n        ids = set()\n    elif type(terms) is dict:\n        ids = set([terms[\"qualifier\"]+\":\"+terms[\"id\"]])\n    else:\n        ids = set((t[\"qualifier\"]+\":\"+t[\"id\"] for t in terms))\n    return ids\n\n\nterms = {\"BP\":set(), \"CC\": set(), \"MF\":set()}\ngene_results = {}\nnf = 0\nfor response in responses:\n    for g in response:\n        if \"notfound\" in g and g[\"notfound\"]: nf += 1\n        if \"go\" not in g: continue\n        bpids = extract(g[\"go\"][\"BP\"] if \"BP\" in g[\"go\"] else set())\n        ccids = extract(g[\"go\"][\"CC\"] if \"CC\" in g[\"go\"] else set())\n        mfids = extract(g[\"go\"][\"MF\"] if \"MF\" in g[\"go\"] else set())\n        g_res = {\"bp\":bpids,\"cc\":ccids,\"mf\":mfids}\n        gene_results[g[\"query\"]] = g_res\n        terms[\"BP\"].update(bpids)\n        terms[\"CC\"].update(ccids)\n        terms[\"MF\"].update(mfids)\n\n\nnf\n\n\nlen(gene_results)\n\n\nlen(terms[\"BP\"])\n\n\nimport matplotlib.pyplot as plt\n\n\nnbp = len(terms[\"BP\"])\nplt.hist([len(g[\"bp\"]) for g in gene_results.values()])\n\n\n\nIssues with Using Gene Ontology\nGene Ontology * Gene products are annotated to the most granular term in the ontology that is supported by the available evidence. * By the transitivity principle, an annotation to a GO term implies annotation to all its parents\n\nhuri\n\n\ngene_id_map = dict(zip(list(gene_ids),range(len(gene_ids))))\n\n\nhuri = huri.assign(ID_A=huri.A.apply(lambda ensg: gene_id_map[ensg]),\n                  ID_B=huri.B.apply(lambda ensg: gene_id_map[ensg]))\n\n\nhuri\n\n\nwith open(\"/data/dzeiberg/ppi/HuRI/HI-union.edgelist\",\"w\") as f:\n    for a,b in zip(huri.ID_A,huri.ID_B):\n        f.write(f\"{a} {b}\\n\")\n        \nwith open(\"/data/dzeiberg/ppi/HuRI/HI-union.weightedEdgeList\",\"w\") as f:\n    for a,b in zip(huri.ID_A,huri.ID_B):\n        f.write(f\"{a} {b} 1\\n\")\n\n\nimport networkx as nx\n\n\nG = nx.read_weighted_edgelist(\"/data/dzeiberg/ppi/HuRI/HI-union.weightedEdgeList\",nodetype=int)\n\n\nimport nxmetis\n\n\n(cut,parts) = nxmetis.partition(G, 2)\n\n\ncut/len(G.edges)\n\n\nG_train,G_val = G.subgraph(parts[0]), G.subgraph(parts[1])\n\n\nlen(G_train.edges),len(G_val.edges)\n\n\nnp.mean([G_train.degree(n) for n in G_train.nodes()])\n\n\nnp.mean([G_val.degree(n) for n in G_val.nodes()])\n\n\nnx.write_edgelist(G_train,\"/data/dzeiberg/ppi/HuRI/HI-union.train.edgelist\")\n\n\nnx.write_edgelist(G_val,\"/data/dzeiberg/ppi/HuRI/HI-union.val.edgelist\")\n\n\n\nRead in Node2Vec\n\nimport pandas as pd\n\n\nn2vTrain = pd.read_csv(\"/data/dzeiberg/ppi/HuRI/HI-union.train.emb\",\n                       delimiter=\" \",\n                       skiprows=[0],header=None,index_col=0)\n\nn2vVal = pd.read_csv(\"/data/dzeiberg/ppi/HuRI/HI-union.val.emb\",\n                       delimiter=\" \",\n                       skiprows=[0],header=None,index_col=0)\n\n\nn2vTrain\n\n\n\nRead in Mutpred2 Results\n\nmp = pd.read_csv(\"data/y2hEdgotyping/mutpred2Results/variants.faa.out\")\nmp = mp.assign(ID=mp.ID.str.replace(\"db_orf_\",\"\"),\n              aa_change_mt=mp.Substitution.apply(lambda s: aaPairs[s[0]]+s[1:-1]+aaPairs[s[-1]]))\n\n\nmp\n\n\nmp.columns\n\n\nmp.loc[0,\"Molecular mechanisms with Pr >= 0.01 and P < 0.99\"].split(\";\")\n\n\n\nGet the file and row number for each variant’s mutpred2 features\n\nu_idx = np.stack((np.ones(len(mp)),\n                  np.zeros(len(mp))),axis=1)\nlast = mp.ID.values[0]\nfor i,v in enumerate(mp.ID.values[1:],start=1):\n    if v == last:\n        u_idx[i,0] = u_idx[i-1,0]\n        u_idx[i,1] = u_idx[i-1,1] + 1\n    else:\n        last = v\n        u_idx[i,0] = u_idx[i-1,0] + 1\n\n\nmp = mp.assign(featFileNum=u_idx[:,0],fileRowNum=u_idx[:,1])\n\n\nmp.ID = mp.ID.astype(int)\n\n\n\nMerge dataframes\n\ndf = pd.merge(y2h,mp,left_on=[\"db_orf_id\",\"aa_change_mt\"],\n        right_on=[\"ID\",\"aa_change_mt\"],validate=\"m:1\")\n\n\ndf\n\n\n#df.to_csv(\"data/y2hEdgotyping/y2hWithMutPred2Info.csv\")\n\n\n\nDefine Score change at level 3\n\ndf = df.assign(delta3=df.LWH25_f_wt - df.LWH25_f_mt)\n\n\n\nLoad Mutpred2 Features for each row\n\ndf.db_n2v_idx.isna().any()\n\n\ngene_id_map\n\n\ndf = df.assign(db_n2v_idx=df.db_ensembl_gene_id_mt.apply(lambda ensg: gene_id_map[ensg]),\n              ad_n2v_idx=df.ad_ensembl_gene_id_mt.apply(lambda ensg: gene_id_map[ensg]))\n\n\nn2vAll = pd.concat((n2vTrain,n2vVal))\n\n\ndef getfold(row):\n    if row.db_n2v_idx in n2vTrain.index and row.ad_n2v_idx in n2vTrain.index:\n        return 0\n    elif row.db_n2v_idx in n2vVal.index and row.ad_n2v_idx in n2vVal.index:\n        return 1\n    return 2\n\n\nfrom scipy.io import loadmat\n\nX = np.zeros((len(df),1345))\nfoldNum = np.zeros(len(df))\nfor idx_i in df.featFileNum.astype(int).unique():\n    idxmask = df.featFileNum == idx_i\n    rownums = df.fileRowNum[idxmask].astype(int)\n    f = loadmat(f\"data/y2hEdgotyping/mutpred2Results/variants.faa.out.feats_{idx_i}\")[\"feats\"]\n    X[idxmask,:] = f[rownums]\n    foldNum[idxmask] = df.loc[idxmask].apply(lambda row: getfold(row),axis=1)\nX2 = n2vAll.loc[df.db_n2v_idx].values\nX3 = n2vAll.loc[df.ad_n2v_idx].values\n\n# X = np.concatenate((X,X2,X3),axis=1)\n\n\nnp.unique(foldNum,return_counts=True)\n\n\n\nDefine Target Task\n\ny = df.delta3 >= 2\n\n\n\nTrain and validate Logistic Regression Model\n\nPredict whether the mutation applied to db, represented by its mutpred2 features, will result in a score change at level 3 with the experiment’s ad.\n\n\nThe limitations here are that the MutPred2 features are independent of the AD protein even though the target value is a function of DB, MT, and AD\n\n\nThe node2vec features are meaningless because the train and validation features are not in the same feature space\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import roc_auc_score\n\n\nclf = RandomForestClassifier(n_estimators=500,n_jobs=20)\n# clf = SVC(probability=True)\n# clf = LogisticRegression()\n\nclf.fit(X[foldNum==0],\n        y[foldNum==0])\n\n\nplt.hist(clf.predict_proba(X[foldNum==0])[:,1])\n\n\nvalpreds = clf.predict_proba(X[foldNum==1])[:,1]\n\n\nplt.hist(valpreds[y[foldNum==1]])\nplt.hist(valpreds[~y[foldNum==1]],alpha=.5,color=\"red\")\n\n\nroc_auc_score(y[foldNum==1],valpreds)"
  },
  {
    "objectID": "nnpu.html",
    "href": "nnpu.html",
    "title": "NNPU",
    "section": "",
    "text": "# default_exp nnpu\n\n\n# export\nfrom sklearn.metrics import roc_auc_score\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import activations\nfrom tensorflow.keras import losses\nimport tensorflow.keras.backend as K\nimport numpy as np\nfrom scipy.stats import bernoulli\nfrom easydict import EasyDict\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt \n\nimport scipy.stats as ss\n\nfrom sklearn.model_selection import train_test_split\n\n\n# export\nclass Basic(tf.keras.Model):\n\n    def __init__(self, n_units, n_hidden, dropout_rate):\n        super(Basic, self).__init__()\n        self.Dens = list()\n        self.BN = list()\n        self.Drop = list()\n        for i in np.arange(n_hidden):\n            if i == 0:\n                self.Dens.append(layers.Dense(n_units, activation='relu'))\n            else:\n                self.Dens.append(layers.Dense(n_units, activation='relu'))\n            self.BN.append(layers.BatchNormalization())\n            self.Drop.append(layers.Dropout(dropout_rate))\n        self.dens_last = layers.Dense(1)\n        # self.BN_last = layers.BatchNormalization()\n        # self.sigmoid = activations.sigmoid()\n\n    def call(self, inputs):\n        for i in np.arange(len(self.Dens)):\n            if i == 0:\n                x = self.Dens[i](inputs)\n            else:\n                x = self.Dens[i](x)\n            x = self.BN[i](x)\n            x = self.Drop[i](x)\n        x = self.dens_last(x)\n        # x = self.BN_last(x)\n        return activations.sigmoid(x)\n\n\n# export\ndef NNPULoss(alpha):\n    epsilon = 10 ** -10\n\n    def loss_function(y_true, pn_posterior):\n        i_zero = K.flatten(tf.equal(y_true, 0))\n        i_one = K.flatten(tf.equal(y_true, 1))\n        pn_posterior_0 = tf.boolean_mask(pn_posterior[:, 0], i_zero, axis=0)\n        pn_posterior_1 = tf.boolean_mask(pn_posterior[:, 0], i_one, axis=0)\n        loss_neg = -tf.reduce_mean(tf.math.log(1 - pn_posterior_0 + epsilon))\n        loss_neg = tf.maximum(0.0, loss_neg + alpha * tf.reduce_mean(tf.math.log(1 - pn_posterior_1 + epsilon)))\n        loss_pos = -alpha * tf.reduce_mean(tf.math.log(pn_posterior_1 + epsilon))\n        return loss_neg + loss_pos\n\n    return loss_function\n\n\ndef NNPUAbsLoss(alpha):\n    epsilon = 10 ** -10\n\n    def loss_function(y_true, pn_posterior):\n        i_zero = K.flatten(tf.equal(y_true, 0))\n        i_one = K.flatten(tf.equal(y_true, 1))\n        pn_posterior_0 = tf.boolean_mask(pn_posterior[:, 0], i_zero, axis=0)\n        pn_posterior_1 = tf.boolean_mask(pn_posterior[:, 0], i_one, axis=0)\n        loss_neg = -tf.reduce_mean(tf.math.log(1 - pn_posterior_0 + epsilon))\n        loss_neg = tf.math.abs(loss_neg + alpha * tf.reduce_mean(tf.math.log(1 - pn_posterior_1 + epsilon)))\n        loss_pos = -alpha * tf.reduce_mean(tf.math.log(pn_posterior_1 + epsilon))\n        return loss_neg + loss_pos\n\n    return loss_function\n\n\n# export\ndef gradients(net, x, y, LossFnc):\n    #YGen = np.cast['float32'](np.concatenate((y,pn_posterior_old, disc_posterior), axis=1))\n    with tf.GradientTape() as tape:\n        #pdb.set_trace()\n        loss = LossFnc(y, net(x))\n    return loss, tape.gradient(loss, net.trainable_variables)\n\n\n# export\ndef batch(x, y, n_p, n_u):\n    x_p, ix_p = batchPos(x, y, n_p)\n    x_u, ix_u = batchUL(x, y, n_u)\n    xx = np.concatenate((x_p, x_u), axis=0)\n    ix = np.concatenate((ix_p, ix_u), axis=0)\n    return xx, y[ix, :], x_p, x_u, ix\n\n\ndef batchPos(x, y, n_p):\n    return batchY(x, y, 1, n_p)\n\n\ndef batchUL(x, y, n_u):\n    return batchY(x, y, 0, n_u)\n\ndef batchY(x, y, value, n, *args):\n    ix = (y == value).flatten( )\n    ix_all = np.arange(np.size(y))\n    ix = ix_all[ix]\n    if args:\n        p = args[0].flatten()\n        p = p[ix]\n        ix_p = bernoulli.rvs(p)\n        ix_p = np.cast['bool'](ix_p)\n        ix = ix[ix_p]\n    ix = np.random.choice(ix, n, replace=True)\n    xx = x[ix, :]\n    return xx, ix\n\n\n# export\ndef getPosterior(x,y,alpha,\n                 inputs=None,\n                 pupost=None,\n                 training_args=EasyDict({\"n_units\":1000,\n                                         \"n_hidden\":10,\n                                         \"dropout_rate\":0.5,\n                                         \"maxIter\":500,\n                                         \"batch_size\":128}),\n                 distributions=None,\n                 viz_freq=10,\n                 plotDistrs=False,\n                absLoss=True,\n                yPN=None):\n    \"\"\"\n    x : (n x d) array\n    y : (n x 1) array\n    alpha : float\n    training_args: EasyDict\n        n_units : default 20 : size of hiddden layers\n        n_hidden : default 10 : number of hidden layers\n        dropout_rate : default 0.1 : drop percentage\n        maxIter : default 100 : number of epochs\n        batch_size : default 500 : batch size\n    distributions : EasyDict :\n        true_posterior(x) : callable\n        f1(x) : callable\n        f0(x) : callable\n    viz_freq : default 10 : if distributions is specified, plot the 1D distributions at this period\n    \"\"\"\n    # model\n    net = Basic(training_args.n_units,\n                training_args.n_hidden,\n                training_args.dropout_rate)\n    # loss\n    if absLoss:\n        LossFnc = NNPUAbsLoss(alpha)\n    else:\n        LossFnc = NNPULoss(alpha)\n    # optimizer\n    opt = tf.keras.optimizers.Adam(learning_rate=1e-4)\n    if pupost is not None:\n        inputs = pupost(x)\n    elif inputs is not None:\n        inputs = inputs\n    else:\n        inputs = x\n    inputsTrain,inputsVal,labelsTrain,labelsVal = train_test_split(inputs,y)\n    \n    def plot():\n        estimatedPosterior = net.predict(inputs)[:,0].ravel()\n        truePosterior = distributions.true_posterior(x).ravel()\n        plt.scatter(estimatedPosterior, truePosterior,alpha=.1,label=\"mae: {:.3f}\".format(np.mean(np.abs(estimatedPosterior - truePosterior))))\n        plt.plot([0,1],[0,1],color=\"black\")\n        plt.xlabel(\"estimated posterior\")\n        plt.ylabel(\"true posterior\")\n        plt.legend()\n        plt.show()\n    minLoss,patience = np.inf,0\n    for i in tqdm(range(training_args.maxIter),total=training_args.maxIter, leave=False):\n        xx,yy,_,_,ix = batch(inputsTrain,labelsTrain,training_args.batch_size,training_args.batch_size)\n        loss, grads = gradients(net,xx,yy,LossFnc)\n        opt.apply_gradients(zip(grads, net.trainable_variables))\n        valloss,_ = gradients(net,inputsVal, labelsVal,LossFnc)\n        if valloss < minLoss:\n            minLoss = valloss\n            patience = 0\n        else:\n            patience += 1\n        if distributions is not None and not i % viz_freq:\n            plot()\n        if patience == 50:\n            break\n    if distributions is not None:\n        plot()\n    return net.predict(inputs),net\n\n\nfrom multiInstanceLearning.data.gaussian_dg import GaussianMixtureDataGenerator\n\n\ndg = GaussianMixtureDataGenerator(64,1,[0.99,.999],1000,10000)\n\n[xPU,yPU,yPN] = dg.pu_data()\n\nxPUTrain,xPUVal,yPUTrain,yPUVal = train_test_split(xPU,yPU,test_size=.2)\n\n\ngetPosterior(xPU,yPU,dg.alpha,\n             distributions=EasyDict({\"true_posterior\" : dg.pn_posterior_cc,\n                                     \"f1\" : dg.dens_pos,\n                                     \"f0\" : dg.dens_neg}),\n             viz_freq=50,\n             absLoss=False,\n             pupost=None,\n             yPN=yPN,\n             training_args=EasyDict({\"n_units\": 1000,\n                                     \"n_hidden\":3,\n                                     \"dropout_rate\":0.5,\n                                     \"maxIter\":10000,\n                                     \"batch_size\":128}),)"
  },
  {
    "objectID": "partition_human_interactome.html",
    "href": "partition_human_interactome.html",
    "title": "Load HuRI",
    "section": "",
    "text": "import networkx as nx\n\n\nimport pandas as pd\n\n\nhuri = pd.read_csv(\"data/HuRI/HI-union.tsv\",\n                   delimiter=\"\\t\",\n                   header=None)\n\n\nhuri\n\n\nMake networkx graph for HuRI\n\nG = nx.from_edgelist(list(zip(huri[0],huri[1])))\n\n\n\nLoad Processed Edgotyping Data\n\ndf = pd.read_csv(\"data/y2hEdgotyping/y2hWithMutPred2Info.csv\",index_col=0)\n\n\ndf\n\n\n\nGet Unique Nodes in Edgotyping Graph\n\nnodes = pd.concat((df[[\"db_ensembl_gene_id_mt\",\"p_seq_db\"]].rename({\"db_ensembl_gene_id_mt\":\"ensembl_gene_id\",\n                                                                       \"p_seq_db\":\"p_seq\"},axis=1),\n             df[[\"ad_ensembl_gene_id_mt\",\"p_seq_ad\"]].rename({\"ad_ensembl_gene_id_mt\":\"ensembl_gene_id\",\n                                                                       \"p_seq_ad\":\"p_seq\"},axis=1))).drop_duplicates().reset_index()\n\n\n\nWrite sequences to csv files in batches of 100 for Uniprot Peptide Search\n\nfor fNum,startIdx in enumerate(np.arange(0,len(nodes),100)):\n    with open(f\"data/y2hEdgotyping/sequences_{fNum}.csv\",\"w\") as f:\n        for idx,n in nodes.iloc[startIdx:startIdx+100].iterrows():\n            f.write(f\"{n.p_seq}\\n\")\n\n\n\nWrite Edgotyping Sequences to fasta for blastp search\n\nwith open(\"data/y2hEdgotyping/sequences.fasta\",\"w\") as f:\n    for idx,(_,ensg,seq) in nodes.iterrows():\n        f.write(f\">{ensg}\\n{seq}\\n\")\n\n\n\nMake Edgotyping networkx graph\n\ny2hNetwork = nx.MultiGraph()\n\nfor idx,row in df[[\"db_ensembl_gene_id_mt\",\"db_orf_id\",\"p_seq_db\",\"symbol_db\"]].drop_duplicates().iterrows():\n    y2hNetwork.add_node(row.db_ensembl_gene_id_mt,orf_id=row.db_orf_id,seq=row.p_seq_db, symbol=row.symbol_db)\n    \nfor idx,row in df[[\"ad_ensembl_gene_id_mt\",\"ad_orf_id\",\"p_seq_ad\",\"symbol_ad\"]].drop_duplicates().iterrows():\n    y2hNetwork.add_node(row.ad_ensembl_gene_id_mt,orf_id=row.ad_orf_id,seq=row.p_seq_ad, symbol=row.symbol_ad)\n\n\ndf\n\n\nfor idx,r in df[[\"db_ensembl_gene_id_mt\", \"ad_ensembl_gene_id_mt\",\"db_mut_id_mt\",\"aa_change_mt\",\n    \"clinical_significance_mt\",\"nt_change_mt\",\"Substitution\",\"MutPred2 score\",\n    'Molecular mechanisms with Pr >= 0.01 and P < 0.99',\n    'Motif information', 'Remarks', 'featFileNum', 'fileRowNum',\n    \"LW_mt\", \"LWH1_f_mt\", \"LWH10_f_mt\", \"LWH25_f_mt\", \"LWA_f_mt\",\"LWAH1_f_mt\",\"score_mt\", \"score_norm_mt\",\n    'LW_wt','LWH1_f_wt', 'LWH10_f_wt', 'LWH25_f_wt', 'LWA_f_wt', 'LWAH1_f_wt',\"score_wt\", \"score_norm_wt\"]].iterrows():\n    y2hNetwork.add_edge(r[0],r[1],**r.to_dict())\n\n\nlen(y2hNetwork.edges)\n\n\nfor e in G.edges():\n    G.edges[e][\"weight\"] = 1 if y2hNetwork.has_edge(*e) else 0\n\n\nsum([e[2][\"weight\"] for e in G.edges.data()])\n\n\nnx.write_gexf(y2hNetwork,\"data/y2hEdgotyping/edgotype.gefx\")\n\n\n\npymetis example\n\nmetisAdj = {1: [2,3,5],\n            2: [1,3,4],\n            3: [1,2,4,5],\n            4: [2,3,6,7],\n            5: [1,3,6],\n            6: [5,4,7],\n            7: [4,6]}\nmetisAdj = {k-1:[vi-1 for vi in v] for k,v in metisAdj.items()}\n\n\nmg = nx.from_dict_of_lists(metisAdj)\n\n\nnx.draw_networkx(mg)\n\n\ndef toMetisFormat(MG):\n    xadj = [0]\n    adjncy = []\n    weight = []\n    nodemap = dict(zip(list(MG.nodes()),range(len(MG.nodes()))))\n    for n in MG.nodes():\n        neighbors = list(MG.neighbors(n))\n        adjncy += [nodemap[ni] for ni in neighbors]\n        xadj.append(len(neighbors) + xadj[-1])\n        edgeweights = []\n        for ni in neighbors:\n            if \"weight\" in MG.edges()[(n,ni)]:\n                edgeweights.append(MG.edges()[(n,ni)][\"weight\"])\n            else:\n                edgeweights.append(1)\n        weight += edgeweights\n    return xadj, adjncy,weight,{v:k for k,v in nodemap.items()}\n\n\nxadj,adjncy,weight,nodemap = toMetisFormat(mg)\n\n\nimport pymetis\n\n\nn_cuts,membership = pymetis.part_graph(2,xadj=xadj,adjncy=adjncy,eweights=weight)\n\n\nn_cuts\n\n\nmembership\n\n\nnp.array([n for n in mg.nodes()])[np.array(membership) == 1]\n\n\n\nApply to HuRI\n\nhu_xadj,hu_adjncy,hu_weight,hu_nodemap = toMetisFormat(G)\n\n\nimport pymetis\n\n\nhu_n_cuts,hu_membership = pymetis.part_graph(3,xadj=hu_xadj,\n                                             adjncy=hu_adjncy,\n                                             eweights=hu_weight)\n\n\nhu_n_cuts\n\n\nnp.unique(hu_membership,return_counts=True)\n\n\ntrainIDs = [hu_nodemap[i] for i in np.argwhere(np.array(hu_membership) == 0).ravel()]\nG_train = G.subgraph(trainIDs)\ny2hNetwork_train = y2hNetwork.subgraph(trainIDs)\n\nvalIDs = [hu_nodemap[i] for i in np.argwhere(np.array(hu_membership) == 1).ravel()]\nG_val = G.subgraph(valIDs)\ny2hNetwork_val = y2hNetwork.subgraph(valIDs)\n\ntestIDs = [hu_nodemap[i] for i in np.argwhere(np.array(hu_membership) == 2).ravel()]\nG_test = G.subgraph(testIDs)\ny2hNetwork_test = y2hNetwork.subgraph(testIDs)\n\n\nlen(G_train.edges),len(G_val.edges), len(G_test.edges)\n\n\nlen(y2hNetwork_train.edges), len(y2hNetwork_val.edges), len(y2hNetwork_test.edges)\n\n\nnx.write_gpickle(G_train,\"data/y2hEdgotyping/HuRI_train.gpickle\")\nnx.write_gpickle(G_val,\"data/y2hEdgotyping/HuRI_val.gpickle\")\nnx.write_gpickle(G_test,\"data/y2hEdgotyping/HuRI_test.gpickle\")\nnx.write_gpickle(y2hNetwork_train,\"data/y2hEdgotyping/edgotype_train.gpickle\")\nnx.write_gpickle(y2hNetwork_val,\"data/y2hEdgotyping/edgotype_val.gpickle\")\nnx.write_gpickle(y2hNetwork_test,\"data/y2hEdgotyping/edgotype_test.gpickle\")"
  }
]